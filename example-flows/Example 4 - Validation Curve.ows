<?xml version='1.0' encoding='utf-8'?>
<scheme version="2.0" title="" description="">
	<nodes>
		<node id="0" name="Paint Data" qualified_name="Orange.widgets.data.owpaintdata.OWPaintData" project_name="Orange3" version="" title="Paint Data" position="(-139.0, -38.0)" />
		<node id="1" name="Select Columns" qualified_name="Orange.widgets.data.owselectcolumns.OWSelectAttributes" project_name="Orange3" version="" title="Select Columns" position="(-40.0, -38.0)" />
		<node id="2" name="Scatter Plot" qualified_name="Orange.widgets.visualize.owscatterplot.OWScatterPlot" project_name="Orange3" version="" title="Scatter Plot" position="(97.0, -107.0)" />
		<node id="3" name="Python Script" qualified_name="Orange.widgets.data.owpythonscript.OWPythonScript" project_name="Orange3" version="" title="Poly_Regression.py (1)" position="(20.0, 99.0)" />
		<node id="4" name="Python Script" qualified_name="Orange.widgets.data.owpythonscript.OWPythonScript" project_name="Orange3" version="" title="Poly_Regression.py (2)" position="(18.0, 214.0)" />
		<node id="5" name="Python Script" qualified_name="Orange.widgets.data.owpythonscript.OWPythonScript" project_name="Orange3" version="" title="Poly_Regression.py (4)" position="(17.0, 439.0)" />
		<node id="6" name="Python Script" qualified_name="Orange.widgets.data.owpythonscript.OWPythonScript" project_name="Orange3" version="" title="Poly_Regression.py (6)" position="(13.0, 662.0)" />
		<node id="7" name="Python Script" qualified_name="Orange.widgets.data.owpythonscript.OWPythonScript" project_name="Orange3" version="" title="Poly_Regression.py (3)" position="(19.0, 322.0)" />
		<node id="8" name="Python Script" qualified_name="Orange.widgets.data.owpythonscript.OWPythonScript" project_name="Orange3" version="" title="Poly_Regression.py (5)" position="(14.0, 552.0)" />
		<node id="9" name="Python Script" qualified_name="Orange.widgets.data.owpythonscript.OWPythonScript" project_name="Orange3" version="" title="Poly_Regression.py (7)" position="(16.0, 770.0)" />
		<node id="10" name="Python Script" qualified_name="Orange.widgets.data.owpythonscript.OWPythonScript" project_name="Orange3" version="" title="Poly_Regression.py (8)" position="(12.0, 886.0)" />
		<node id="11" name="Python Script" qualified_name="Orange.widgets.data.owpythonscript.OWPythonScript" project_name="Orange3" version="" title="Validation_curve.py " position="(334.0, -12.0)" />
	</nodes>
	<links>
		<link id="0" source_node_id="0" sink_node_id="1" source_channel="Data" sink_channel="Data" enabled="true" />
		<link id="1" source_node_id="1" sink_node_id="2" source_channel="Data" sink_channel="Data" enabled="true" />
		<link id="2" source_node_id="3" sink_node_id="11" source_channel="Learner" sink_channel="Learner" enabled="true" />
		<link id="3" source_node_id="4" sink_node_id="11" source_channel="Learner" sink_channel="Learner" enabled="true" />
		<link id="4" source_node_id="7" sink_node_id="11" source_channel="Learner" sink_channel="Learner" enabled="true" />
		<link id="5" source_node_id="5" sink_node_id="11" source_channel="Learner" sink_channel="Learner" enabled="true" />
		<link id="6" source_node_id="8" sink_node_id="11" source_channel="Learner" sink_channel="Learner" enabled="true" />
		<link id="7" source_node_id="6" sink_node_id="11" source_channel="Learner" sink_channel="Learner" enabled="true" />
		<link id="8" source_node_id="9" sink_node_id="11" source_channel="Learner" sink_channel="Learner" enabled="true" />
		<link id="9" source_node_id="10" sink_node_id="11" source_channel="Learner" sink_channel="Learner" enabled="true" />
		<link id="10" source_node_id="1" sink_node_id="3" source_channel="Data" sink_channel="Data" enabled="true" />
		<link id="11" source_node_id="1" sink_node_id="4" source_channel="Data" sink_channel="Data" enabled="true" />
		<link id="12" source_node_id="1" sink_node_id="7" source_channel="Data" sink_channel="Data" enabled="true" />
		<link id="13" source_node_id="1" sink_node_id="5" source_channel="Data" sink_channel="Data" enabled="true" />
		<link id="14" source_node_id="1" sink_node_id="8" source_channel="Data" sink_channel="Data" enabled="true" />
		<link id="15" source_node_id="1" sink_node_id="6" source_channel="Data" sink_channel="Data" enabled="true" />
		<link id="16" source_node_id="1" sink_node_id="9" source_channel="Data" sink_channel="Data" enabled="true" />
		<link id="17" source_node_id="1" sink_node_id="10" source_channel="Data" sink_channel="Data" enabled="true" />
		<link id="18" source_node_id="1" sink_node_id="11" source_channel="Data" sink_channel="Data" enabled="true" />
	</links>
	<annotations>
		<text id="0" type="text/markdown" rect="(-187.0, -238.0, 573.0, 62.0)" font-family="MS Shell Dlg 2" font-size="16"># Example for Validation Curve
</text>
		<text id="1" type="text/plain" rect="(455.0, -48.0, 233.0, 88.0)" font-family="MS Shell Dlg 2" font-size="16">see the validation curve 
(no replicable training; so several runs give different results)</text>
		<arrow id="2" start="(450.0, -15.0)" end="(374.0, -13.0)" fill="#C1272D" />
		<arrow id="3" start="(335.0, 244.0)" end="(66.0, 112.0)" fill="#C1272D" />
		<text id="4" type="text/plain" rect="(351.0, 219.0, 298.0, 109.0)" font-family="MS Shell Dlg 2" font-size="16">see the name property;
this name is uses as x-label in the diagram</text>
	</annotations>
	<thumbnail />
	<node_properties>
		<properties node_id="0" format="literal">{'attr1': 'x', 'attr2': 'y', 'autocommit': True, 'brushRadius': 75, 'controlAreaVisible': True, 'data': [[0.010783383940286703, 0.2975030783573148, 0.0], [0.03986075344251205, 0.39217818853493247, 0.0], [0.1034096671190359, 0.580015004946324, 0.0], [0.2001679297338846, 0.6442898508125617, 0.0], [0.24012517160500768, 0.6252587096432315, 0.0], [0.1765345837004807, 0.6883116330005694, 0.0], [0.319139273272614, 0.5503320615590699, 0.0], [0.2727175837559706, 0.5698915971509196, 0.0], [0.36302961379158316, 0.414457345153919, 0.0], [0.47255476747510516, 0.32480575096127523, 0.0], [0.5123974137789061, 0.31494590794886507, 0.0], [0.5177860889287595, 0.23531369187884774, 0.0], [0.5968418734706349, 0.24677082096034675, 0.0], [0.699690850241529, 0.2953692189430633, 0.0], [0.7637234000537376, 0.3688185878777299, 0.0], [0.7611171450286315, 0.4646600100832098, 0.0], [0.7958099741137518, 0.5177069716354361, 0.0], [0.8353259404327023, 0.7506503178367862, 0.0], [0.94303129513231, 0.8331918196557742, 0.0], [0.9081110937298393, 0.8642934317010036, 0.0]], 'density': 1, 'hasAttr2': True, 'labels': ['C1', 'C2'], 'savedWidgetGeometry': b'\x01\xd9\xd0\xcb\x00\x03\x00\x00\x00\x00\x00\x0c\x00\x00\x00+\x00\x00\x04w\x00\x00\x03\x07\x00\x00\x00\x0c\x00\x00\x00+\x00\x00\x04w\x00\x00\x03\x07\x00\x00\x00\x00\x00\x00\x00\x00\x07\x80\x00\x00\x00\x0c\x00\x00\x00+\x00\x00\x04w\x00\x00\x03\x07', 'symbol_size': 10, 'table_name': 'Painted data', '__version__': 1}</properties>
		<properties node_id="1" format="pickle">gASVgwEAAAAAAAB9lCiMC2F1dG9fY29tbWl0lIiMEmNvbnRyb2xBcmVhVmlzaWJsZZSIjBNpZ25v
cmVfbmV3X2ZlYXR1cmVzlImME3NhdmVkV2lkZ2V0R2VvbWV0cnmUQ0IB2dDLAAMAAAAAAJcAAADK
AAADhQAAAsgAAACYAAAA6QAAA4QAAALHAAAAAAAAAAAHgAAAAJgAAADpAAADhAAAAseUjBJ1c2Vf
aW5wdXRfZmVhdHVyZXOUiYwLX192ZXJzaW9uX1+USwGMEGNvbnRleHRfc2V0dGluZ3OUXZSMFW9y
YW5nZXdpZGdldC5zZXR0aW5nc5SMB0NvbnRleHSUk5QpgZR9lCiMBnZhbHVlc5R9lCiMEWRvbWFp
bl9yb2xlX2hpbnRzlH2UKIwBeJRLAoaUjAlhdHRyaWJ1dGWUSwCGlIwBeZRLAoaUjAVjbGFzc5RL
AIaUdUr+////hpRoB0sBdYwKYXR0cmlidXRlc5R9lChoE0sCaBdLAnWMBW1ldGFzlH2UdWJhdS4=
</properties>
		<properties node_id="2" format="pickle">gASViQIAAAAAAAB9lCiMC2F1dG9fY29tbWl0lIiMC2F1dG9fc2FtcGxllIiMEmNvbnRyb2xBcmVh
VmlzaWJsZZSIjBNzYXZlZFdpZGdldEdlb21ldHJ5lENCAdnQywADAAAAAAMEAAABFwAAB3EAAAP6
AAADBQAAATYAAAdwAAAD+QAAAAAAAAAAB4AAAAMFAAABNgAAB3AAAAP5lIwJc2VsZWN0aW9ulE6M
EXRvb2x0aXBfc2hvd3NfYWxslIiMD3Zpc3VhbF9zZXR0aW5nc5R9lIwFZ3JhcGiUfZQojAthbHBo
YV92YWx1ZZRLgIwNY2xhc3NfZGVuc2l0eZSJjBFqaXR0ZXJfY29udGludW91c5SJjAtqaXR0ZXJf
c2l6ZZRLAIwTbGFiZWxfb25seV9zZWxlY3RlZJSJjBZvcnRob25vcm1hbF9yZWdyZXNzaW9ulImM
C3BvaW50X3dpZHRolEsKjAlzaG93X2dyaWSUiYwLc2hvd19sZWdlbmSUiIwNc2hvd19yZWdfbGlu
ZZSJdYwLX192ZXJzaW9uX1+USwWMEGNvbnRleHRfc2V0dGluZ3OUXZSMFW9yYW5nZXdpZGdldC5z
ZXR0aW5nc5SMB0NvbnRleHSUk5QpgZR9lCiMBnZhbHVlc5R9lCiMCmF0dHJfY29sb3KUTkr+////
hpSMCmF0dHJfbGFiZWyUTkr+////hpSMCmF0dHJfc2hhcGWUTkr+////hpSMCWF0dHJfc2l6ZZRO
Sv7///+GlIwGYXR0cl94lIwBeJRLZoaUjAZhdHRyX3mUjAF5lEtmhpRoCn2UaBZLBXWMCmF0dHJp
YnV0ZXOUfZQoaClLAmgsSwJ1jAVtZXRhc5R9lHViYXUu
</properties>
		<properties node_id="3" format="literal">{'controlAreaVisible': True, 'currentScriptIndex': 0, 'savedWidgetGeometry': b'\x01\xd9\xd0\xcb\x00\x03\x00\x00\x00\x00\x00x\x00\x00\x00.\x00\x00\x05v\x00\x00\x03\xe0\x00\x00\x00x\x00\x00\x00.\x00\x00\x05v\x00\x00\x03\xe0\x00\x00\x00\x00\x00\x00\x00\x00\x07\x80\x00\x00\x00x\x00\x00\x00.\x00\x00\x05v\x00\x00\x03\xe0', 'scriptLibrary': [{'name': 'Poly_Regression.py', 'script': '# Polynomial Regression\n########################################################\n# Settings:\nname="1"         # Name of the learner/model in other widgets, e.g. Test and Score\ndegree = 1            # maximal degree of the polynomial features\nfit_intercept = True  # \ntype = "0"            # 0: ordinary Least square \n                      # 1: Lasso (L1-Regularization)\n                      # 2: Ridge (L2-Regularization)\nalpha = 0.0001        # parameter alpha for L1- and L2-Regularization only\n#################################################################\n# File: Poly_Regression.py\n\n"""\n* Widget input: \n    - (optional) data \n    - (optional) preprocessors on object-input\n\n* Widget output: \n    - learner on learner-output, if no data on input\n    - learner and trained model on classifier-output, if data on input\n\n* Creates a learner for polynomial regression, which can be used e.g. in Test and Score-Widget\n* Additionally creates a trained model, if training data is connectet to data-input, e.g. for Predictions-Widget\n\n"""\n\nimport Orange\ndata = in_data\npreprocessors=in_objects\n\nif type == "0":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LinearRegressionLearner(fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: ordinary Least square")\n\nelif type == "1":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LassoRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept, max_iter=2000),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Lasso (L1)")\n\nelif type == "2":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.RidgeRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Ridge (L2)")\nelse:\n    print("no algorism specified")\n\nlearner.name = name\n\nif data != None:\n    classifier = learner(data)  \n    classifier.name = name\n\nout_learner = learner\nif data != None:\n    out_classifier = classifier \nelse:\n    out_classifier = None\nprint("\\nPreprocessors:")\nprint(preprocessors)\n\n\n# all possible settings for underlying learners for linear regression:\n#Orange.regression.linear.LinearRegressionLearner(preprocessors=None, fit_intercept=True)\n\n#Orange.regression.linear.RidgeRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=\'auto\', preprocessors=None)\n\n#Orange.regression.linear.LassoRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, preprocessors=None)\n', 'filename': 'C:/Users/00613/Sync-Austausch/_ML/_Orange/Orange-Python-Scripts/PolyFeatures.py'}], 'scriptText': '# Polynomial Regression\n########################################################\n# Settings:\nname="1"         # Name of the learner/model in other widgets, e.g. Test and Score\ndegree = 1            # maximal degree of the polynomial features\nfit_intercept = True  # \ntype = "0"            # 0: ordinary Least square \n                      # 1: Lasso (L1-Regularization)\n                      # 2: Ridge (L2-Regularization)\nalpha = 0.0001        # parameter alpha for L1- and L2-Regularization only\n#################################################################\n# File: Poly_Regression.py\n\n"""\n* Widget input: \n    - (optional) data \n    - (optional) preprocessors on object-input\n\n* Widget output: \n    - learner on learner-output, if no data on input\n    - learner and trained model on classifier-output, if data on input\n\n* Creates a learner for polynomial regression, which can be used e.g. in Test and Score-Widget\n* Additionally creates a trained model, if training data is connectet to data-input, e.g. for Predictions-Widget\n\n"""\n\nimport Orange\ndata = in_data\npreprocessors=in_objects\n\nif type == "0":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LinearRegressionLearner(fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: ordinary Least square")\n\nelif type == "1":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LassoRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept, max_iter=2000),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Lasso (L1)")\n\nelif type == "2":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.RidgeRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Ridge (L2)")\nelse:\n    print("no algorism specified")\n\nlearner.name = name\n\nif data != None:\n    classifier = learner(data)  \n    classifier.name = name\n\nout_learner = learner\nif data != None:\n    out_classifier = classifier \nelse:\n    out_classifier = None\nprint("\\nPreprocessors:")\nprint(preprocessors)\n\n\n# all possible settings for underlying learners for linear regression:\n#Orange.regression.linear.LinearRegressionLearner(preprocessors=None, fit_intercept=True)\n\n#Orange.regression.linear.RidgeRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=\'auto\', preprocessors=None)\n\n#Orange.regression.linear.LassoRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, preprocessors=None)\n', 'splitterState': b'\x00\x00\x00\xff\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x02\xbd\x00\x00\x00\xce\x01\xff\xff\xff\xff\x01\x00\x00\x00\x02\x00', 'vimModeEnabled': False, '__version__': 2}</properties>
		<properties node_id="4" format="literal">{'controlAreaVisible': True, 'currentScriptIndex': 0, 'savedWidgetGeometry': b'\x01\xd9\xd0\xcb\x00\x03\x00\x00\x00\x00\x00w\x00\x00\x00\x0f\x00\x00\x05w\x00\x00\x03\xe1\x00\x00\x00x\x00\x00\x00.\x00\x00\x05v\x00\x00\x03\xe0\x00\x00\x00\x00\x00\x00\x00\x00\x07\x80\x00\x00\x00x\x00\x00\x00.\x00\x00\x05v\x00\x00\x03\xe0', 'scriptLibrary': [{'name': 'Poly_Regression.py', 'script': '# Polynomial Regression\n########################################################\n# Settings:\nname="1"         # Name of the learner/model in other widgets, e.g. Test and Score\ndegree = 1            # maximal degree of the polynomial features\nfit_intercept = True  # \ntype = "0"            # 0: ordinary Least square \n                      # 1: Lasso (L1-Regularization)\n                      # 2: Ridge (L2-Regularization)\nalpha = 0.0001        # parameter alpha for L1- and L2-Regularization only\n#################################################################\n# File: Poly_Regression.py\n\n"""\n* Widget input: \n    - (optional) data \n    - (optional) preprocessors on object-input\n\n* Widget output: \n    - learner on learner-output, if no data on input\n    - learner and trained model on classifier-output, if data on input\n\n* Creates a learner for polynomial regression, which can be used e.g. in Test and Score-Widget\n* Additionally creates a trained model, if training data is connectet to data-input, e.g. for Predictions-Widget\n\n"""\n\nimport Orange\ndata = in_data\npreprocessors=in_objects\n\nif type == "0":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LinearRegressionLearner(fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: ordinary Least square")\n\nelif type == "1":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LassoRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept, max_iter=2000),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Lasso (L1)")\n\nelif type == "2":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.RidgeRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Ridge (L2)")\nelse:\n    print("no algorism specified")\n\nlearner.name = name\n\nif data != None:\n    classifier = learner(data)  \n    classifier.name = name\n\nout_learner = learner\nif data != None:\n    out_classifier = classifier \nelse:\n    out_classifier = None\nprint("\\nPreprocessors:")\nprint(preprocessors)\n\n\n# all possible settings for underlying learners for linear regression:\n#Orange.regression.linear.LinearRegressionLearner(preprocessors=None, fit_intercept=True)\n\n#Orange.regression.linear.RidgeRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=\'auto\', preprocessors=None)\n\n#Orange.regression.linear.LassoRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, preprocessors=None)\n', 'filename': 'C:/Users/00613/Sync-Austausch/_ML/_Orange/Orange-Python-Scripts/PolyFeatures.py'}], 'scriptText': '# Polynomial Regression\n########################################################\n# Settings:\nname="2"         # Name of the learner/model in other widgets, e.g. Test and Score\ndegree = 2            # maximal degree of the polynomial features\nfit_intercept = True  # \ntype = "0"            # 0: ordinary Least square \n                      # 1: Lasso (L1-Regularization)\n                      # 2: Ridge (L2-Regularization)\nalpha = 0.0001        # parameter alpha for L1- and L2-Regularization only\n#################################################################\n# File: Poly_Regression.py\n\n"""\n* Widget input: \n    - (optional) data \n    - (optional) preprocessors on object-input\n\n* Widget output: \n    - learner on learner-output, if no data on input\n    - learner and trained model on classifier-output, if data on input\n\n* Creates a learner for polynomial regression, which can be used e.g. in Test and Score-Widget\n* Additionally creates a trained model, if training data is connectet to data-input, e.g. for Predictions-Widget\n\n"""\n\nimport Orange\ndata = in_data\npreprocessors=in_objects\n\nif type == "0":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LinearRegressionLearner(fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: ordinary Least square")\n\nelif type == "1":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LassoRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept, max_iter=2000),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Lasso (L1)")\n\nelif type == "2":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.RidgeRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Ridge (L2)")\nelse:\n    print("no algorism specified")\n\nlearner.name = name\n\nif data != None:\n    classifier = learner(data)  \n    classifier.name = name\n\nout_learner = learner\nif data != None:\n    out_classifier = classifier \nelse:\n    out_classifier = None\nprint("\\nPreprocessors:")\nprint(preprocessors)\n\n\n# all possible settings for underlying learners for linear regression:\n#Orange.regression.linear.LinearRegressionLearner(preprocessors=None, fit_intercept=True)\n\n#Orange.regression.linear.RidgeRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=\'auto\', preprocessors=None)\n\n#Orange.regression.linear.LassoRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, preprocessors=None)\n', 'splitterState': b'\x00\x00\x00\xff\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x02\xbd\x00\x00\x00\xce\x01\xff\xff\xff\xff\x01\x00\x00\x00\x02\x00', 'vimModeEnabled': False, '__version__': 2}</properties>
		<properties node_id="5" format="literal">{'controlAreaVisible': True, 'currentScriptIndex': 0, 'savedWidgetGeometry': b'\x01\xd9\xd0\xcb\x00\x03\x00\x00\x00\x00\x00w\x00\x00\x00\x0f\x00\x00\x05w\x00\x00\x03\xe1\x00\x00\x00x\x00\x00\x00.\x00\x00\x05v\x00\x00\x03\xe0\x00\x00\x00\x00\x00\x00\x00\x00\x07\x80\x00\x00\x00x\x00\x00\x00.\x00\x00\x05v\x00\x00\x03\xe0', 'scriptLibrary': [{'name': 'Poly_Regression.py', 'script': '# Polynomial Regression\n########################################################\n# Settings:\nname="1"         # Name of the learner/model in other widgets, e.g. Test and Score\ndegree = 1            # maximal degree of the polynomial features\nfit_intercept = True  # \ntype = "0"            # 0: ordinary Least square \n                      # 1: Lasso (L1-Regularization)\n                      # 2: Ridge (L2-Regularization)\nalpha = 0.0001        # parameter alpha for L1- and L2-Regularization only\n#################################################################\n# File: Poly_Regression.py\n\n"""\n* Widget input: \n    - (optional) data \n    - (optional) preprocessors on object-input\n\n* Widget output: \n    - learner on learner-output, if no data on input\n    - learner and trained model on classifier-output, if data on input\n\n* Creates a learner for polynomial regression, which can be used e.g. in Test and Score-Widget\n* Additionally creates a trained model, if training data is connectet to data-input, e.g. for Predictions-Widget\n\n"""\n\nimport Orange\ndata = in_data\npreprocessors=in_objects\n\nif type == "0":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LinearRegressionLearner(fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: ordinary Least square")\n\nelif type == "1":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LassoRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept, max_iter=2000),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Lasso (L1)")\n\nelif type == "2":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.RidgeRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Ridge (L2)")\nelse:\n    print("no algorism specified")\n\nlearner.name = name\n\nif data != None:\n    classifier = learner(data)  \n    classifier.name = name\n\nout_learner = learner\nif data != None:\n    out_classifier = classifier \nelse:\n    out_classifier = None\nprint("\\nPreprocessors:")\nprint(preprocessors)\n\n\n# all possible settings for underlying learners for linear regression:\n#Orange.regression.linear.LinearRegressionLearner(preprocessors=None, fit_intercept=True)\n\n#Orange.regression.linear.RidgeRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=\'auto\', preprocessors=None)\n\n#Orange.regression.linear.LassoRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, preprocessors=None)\n', 'filename': 'C:/Users/00613/Sync-Austausch/_ML/_Orange/Orange-Python-Scripts/PolyFeatures.py'}], 'scriptText': '# Polynomial Regression\n########################################################\n# Settings:\nname="4"         # Name of the learner/model in other widgets, e.g. Test and Score\ndegree = 4            # maximal degree of the polynomial features\nfit_intercept = True  # \ntype = "0"            # 0: ordinary Least square \n                      # 1: Lasso (L1-Regularization)\n                      # 2: Ridge (L2-Regularization)\nalpha = 0.0001        # parameter alpha for L1- and L2-Regularization only\n#################################################################\n# File: Poly_Regression.py\n\n"""\n* Widget input: \n    - (optional) data \n    - (optional) preprocessors on object-input\n\n* Widget output: \n    - learner on learner-output, if no data on input\n    - learner and trained model on classifier-output, if data on input\n\n* Creates a learner for polynomial regression, which can be used e.g. in Test and Score-Widget\n* Additionally creates a trained model, if training data is connectet to data-input, e.g. for Predictions-Widget\n\n"""\n\nimport Orange\ndata = in_data\npreprocessors=in_objects\n\nif type == "0":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LinearRegressionLearner(fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: ordinary Least square")\n\nelif type == "1":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LassoRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept, max_iter=2000),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Lasso (L1)")\n\nelif type == "2":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.RidgeRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Ridge (L2)")\nelse:\n    print("no algorism specified")\n\nlearner.name = name\n\nif data != None:\n    classifier = learner(data)  \n    classifier.name = name\n\nout_learner = learner\nif data != None:\n    out_classifier = classifier \nelse:\n    out_classifier = None\nprint("\\nPreprocessors:")\nprint(preprocessors)\n\n\n# all possible settings for underlying learners for linear regression:\n#Orange.regression.linear.LinearRegressionLearner(preprocessors=None, fit_intercept=True)\n\n#Orange.regression.linear.RidgeRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=\'auto\', preprocessors=None)\n\n#Orange.regression.linear.LassoRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, preprocessors=None)\n', 'splitterState': b'\x00\x00\x00\xff\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x02\xbd\x00\x00\x00\xce\x01\xff\xff\xff\xff\x01\x00\x00\x00\x02\x00', 'vimModeEnabled': False, '__version__': 2}</properties>
		<properties node_id="6" format="literal">{'controlAreaVisible': True, 'currentScriptIndex': 0, 'savedWidgetGeometry': b'\x01\xd9\xd0\xcb\x00\x03\x00\x00\x00\x00\x00w\x00\x00\x00\x0f\x00\x00\x05w\x00\x00\x03\xe1\x00\x00\x00x\x00\x00\x00.\x00\x00\x05v\x00\x00\x03\xe0\x00\x00\x00\x00\x00\x00\x00\x00\x07\x80\x00\x00\x00x\x00\x00\x00.\x00\x00\x05v\x00\x00\x03\xe0', 'scriptLibrary': [{'name': 'Poly_Regression.py', 'script': '# Polynomial Regression\n########################################################\n# Settings:\nname="1"         # Name of the learner/model in other widgets, e.g. Test and Score\ndegree = 1            # maximal degree of the polynomial features\nfit_intercept = True  # \ntype = "0"            # 0: ordinary Least square \n                      # 1: Lasso (L1-Regularization)\n                      # 2: Ridge (L2-Regularization)\nalpha = 0.0001        # parameter alpha for L1- and L2-Regularization only\n#################################################################\n# File: Poly_Regression.py\n\n"""\n* Widget input: \n    - (optional) data \n    - (optional) preprocessors on object-input\n\n* Widget output: \n    - learner on learner-output, if no data on input\n    - learner and trained model on classifier-output, if data on input\n\n* Creates a learner for polynomial regression, which can be used e.g. in Test and Score-Widget\n* Additionally creates a trained model, if training data is connectet to data-input, e.g. for Predictions-Widget\n\n"""\n\nimport Orange\ndata = in_data\npreprocessors=in_objects\n\nif type == "0":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LinearRegressionLearner(fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: ordinary Least square")\n\nelif type == "1":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LassoRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept, max_iter=2000),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Lasso (L1)")\n\nelif type == "2":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.RidgeRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Ridge (L2)")\nelse:\n    print("no algorism specified")\n\nlearner.name = name\n\nif data != None:\n    classifier = learner(data)  \n    classifier.name = name\n\nout_learner = learner\nif data != None:\n    out_classifier = classifier \nelse:\n    out_classifier = None\nprint("\\nPreprocessors:")\nprint(preprocessors)\n\n\n# all possible settings for underlying learners for linear regression:\n#Orange.regression.linear.LinearRegressionLearner(preprocessors=None, fit_intercept=True)\n\n#Orange.regression.linear.RidgeRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=\'auto\', preprocessors=None)\n\n#Orange.regression.linear.LassoRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, preprocessors=None)\n', 'filename': 'C:/Users/00613/Sync-Austausch/_ML/_Orange/Orange-Python-Scripts/PolyFeatures.py'}], 'scriptText': '# Polynomial Regression\n########################################################\n# Settings:\nname="6"         # Name of the learner/model in other widgets, e.g. Test and Score\ndegree = 6            # maximal degree of the polynomial features\nfit_intercept = True  # \ntype = "0"            # 0: ordinary Least square \n                      # 1: Lasso (L1-Regularization)\n                      # 2: Ridge (L2-Regularization)\nalpha = 0.0001        # parameter alpha for L1- and L2-Regularization only\n#################################################################\n# File: Poly_Regression.py\n\n"""\n* Widget input: \n    - (optional) data \n    - (optional) preprocessors on object-input\n\n* Widget output: \n    - learner on learner-output, if no data on input\n    - learner and trained model on classifier-output, if data on input\n\n* Creates a learner for polynomial regression, which can be used e.g. in Test and Score-Widget\n* Additionally creates a trained model, if training data is connectet to data-input, e.g. for Predictions-Widget\n\n"""\n\nimport Orange\ndata = in_data\npreprocessors=in_objects\n\nif type == "0":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LinearRegressionLearner(fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: ordinary Least square")\n\nelif type == "1":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LassoRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept, max_iter=2000),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Lasso (L1)")\n\nelif type == "2":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.RidgeRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Ridge (L2)")\nelse:\n    print("no algorism specified")\n\nlearner.name = name\n\nif data != None:\n    classifier = learner(data)  \n    classifier.name = name\n\nout_learner = learner\nif data != None:\n    out_classifier = classifier \nelse:\n    out_classifier = None\nprint("\\nPreprocessors:")\nprint(preprocessors)\n\n\n# all possible settings for underlying learners for linear regression:\n#Orange.regression.linear.LinearRegressionLearner(preprocessors=None, fit_intercept=True)\n\n#Orange.regression.linear.RidgeRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=\'auto\', preprocessors=None)\n\n#Orange.regression.linear.LassoRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, preprocessors=None)\n', 'splitterState': b'\x00\x00\x00\xff\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x02\xbd\x00\x00\x00\xce\x01\xff\xff\xff\xff\x01\x00\x00\x00\x02\x00', 'vimModeEnabled': False, '__version__': 2}</properties>
		<properties node_id="7" format="literal">{'controlAreaVisible': True, 'currentScriptIndex': 0, 'savedWidgetGeometry': b'\x01\xd9\xd0\xcb\x00\x03\x00\x00\x00\x00\x00w\x00\x00\x00\x0f\x00\x00\x05w\x00\x00\x03\xe1\x00\x00\x00x\x00\x00\x00.\x00\x00\x05v\x00\x00\x03\xe0\x00\x00\x00\x00\x00\x00\x00\x00\x07\x80\x00\x00\x00x\x00\x00\x00.\x00\x00\x05v\x00\x00\x03\xe0', 'scriptLibrary': [{'name': 'Poly_Regression.py', 'script': '# Polynomial Regression\n########################################################\n# Settings:\nname="1"         # Name of the learner/model in other widgets, e.g. Test and Score\ndegree = 1            # maximal degree of the polynomial features\nfit_intercept = True  # \ntype = "0"            # 0: ordinary Least square \n                      # 1: Lasso (L1-Regularization)\n                      # 2: Ridge (L2-Regularization)\nalpha = 0.0001        # parameter alpha for L1- and L2-Regularization only\n#################################################################\n# File: Poly_Regression.py\n\n"""\n* Widget input: \n    - (optional) data \n    - (optional) preprocessors on object-input\n\n* Widget output: \n    - learner on learner-output, if no data on input\n    - learner and trained model on classifier-output, if data on input\n\n* Creates a learner for polynomial regression, which can be used e.g. in Test and Score-Widget\n* Additionally creates a trained model, if training data is connectet to data-input, e.g. for Predictions-Widget\n\n"""\n\nimport Orange\ndata = in_data\npreprocessors=in_objects\n\nif type == "0":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LinearRegressionLearner(fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: ordinary Least square")\n\nelif type == "1":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LassoRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept, max_iter=2000),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Lasso (L1)")\n\nelif type == "2":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.RidgeRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Ridge (L2)")\nelse:\n    print("no algorism specified")\n\nlearner.name = name\n\nif data != None:\n    classifier = learner(data)  \n    classifier.name = name\n\nout_learner = learner\nif data != None:\n    out_classifier = classifier \nelse:\n    out_classifier = None\nprint("\\nPreprocessors:")\nprint(preprocessors)\n\n\n# all possible settings for underlying learners for linear regression:\n#Orange.regression.linear.LinearRegressionLearner(preprocessors=None, fit_intercept=True)\n\n#Orange.regression.linear.RidgeRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=\'auto\', preprocessors=None)\n\n#Orange.regression.linear.LassoRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, preprocessors=None)\n', 'filename': 'C:/Users/00613/Sync-Austausch/_ML/_Orange/Orange-Python-Scripts/PolyFeatures.py'}], 'scriptText': '# Polynomial Regression\n########################################################\n# Settings:\nname="3"         # Name of the learner/model in other widgets, e.g. Test and Score\ndegree = 3            # maximal degree of the polynomial features\nfit_intercept = True  # \ntype = "0"            # 0: ordinary Least square \n                      # 1: Lasso (L1-Regularization)\n                      # 2: Ridge (L2-Regularization)\nalpha = 0.0001        # parameter alpha for L1- and L2-Regularization only\n#################################################################\n# File: Poly_Regression.py\n\n"""\n* Widget input: \n    - (optional) data \n    - (optional) preprocessors on object-input\n\n* Widget output: \n    - learner on learner-output, if no data on input\n    - learner and trained model on classifier-output, if data on input\n\n* Creates a learner for polynomial regression, which can be used e.g. in Test and Score-Widget\n* Additionally creates a trained model, if training data is connectet to data-input, e.g. for Predictions-Widget\n\n"""\n\nimport Orange\ndata = in_data\npreprocessors=in_objects\n\nif type == "0":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LinearRegressionLearner(fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: ordinary Least square")\n\nelif type == "1":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LassoRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept, max_iter=2000),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Lasso (L1)")\n\nelif type == "2":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.RidgeRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Ridge (L2)")\nelse:\n    print("no algorism specified")\n\nlearner.name = name\n\nif data != None:\n    classifier = learner(data)  \n    classifier.name = name\n\nout_learner = learner\nif data != None:\n    out_classifier = classifier \nelse:\n    out_classifier = None\nprint("\\nPreprocessors:")\nprint(preprocessors)\n\n\n# all possible settings for underlying learners for linear regression:\n#Orange.regression.linear.LinearRegressionLearner(preprocessors=None, fit_intercept=True)\n\n#Orange.regression.linear.RidgeRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=\'auto\', preprocessors=None)\n\n#Orange.regression.linear.LassoRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, preprocessors=None)\n', 'splitterState': b'\x00\x00\x00\xff\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x02\xbd\x00\x00\x00\xce\x01\xff\xff\xff\xff\x01\x00\x00\x00\x02\x00', 'vimModeEnabled': False, '__version__': 2}</properties>
		<properties node_id="8" format="literal">{'controlAreaVisible': True, 'currentScriptIndex': 0, 'savedWidgetGeometry': b'\x01\xd9\xd0\xcb\x00\x03\x00\x00\x00\x00\x00w\x00\x00\x00\x0f\x00\x00\x05w\x00\x00\x03\xe1\x00\x00\x00x\x00\x00\x00.\x00\x00\x05v\x00\x00\x03\xe0\x00\x00\x00\x00\x00\x00\x00\x00\x07\x80\x00\x00\x00x\x00\x00\x00.\x00\x00\x05v\x00\x00\x03\xe0', 'scriptLibrary': [{'name': 'Poly_Regression.py', 'script': '# Polynomial Regression\n########################################################\n# Settings:\nname="1"         # Name of the learner/model in other widgets, e.g. Test and Score\ndegree = 1            # maximal degree of the polynomial features\nfit_intercept = True  # \ntype = "0"            # 0: ordinary Least square \n                      # 1: Lasso (L1-Regularization)\n                      # 2: Ridge (L2-Regularization)\nalpha = 0.0001        # parameter alpha for L1- and L2-Regularization only\n#################################################################\n# File: Poly_Regression.py\n\n"""\n* Widget input: \n    - (optional) data \n    - (optional) preprocessors on object-input\n\n* Widget output: \n    - learner on learner-output, if no data on input\n    - learner and trained model on classifier-output, if data on input\n\n* Creates a learner for polynomial regression, which can be used e.g. in Test and Score-Widget\n* Additionally creates a trained model, if training data is connectet to data-input, e.g. for Predictions-Widget\n\n"""\n\nimport Orange\ndata = in_data\npreprocessors=in_objects\n\nif type == "0":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LinearRegressionLearner(fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: ordinary Least square")\n\nelif type == "1":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LassoRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept, max_iter=2000),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Lasso (L1)")\n\nelif type == "2":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.RidgeRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Ridge (L2)")\nelse:\n    print("no algorism specified")\n\nlearner.name = name\n\nif data != None:\n    classifier = learner(data)  \n    classifier.name = name\n\nout_learner = learner\nif data != None:\n    out_classifier = classifier \nelse:\n    out_classifier = None\nprint("\\nPreprocessors:")\nprint(preprocessors)\n\n\n# all possible settings for underlying learners for linear regression:\n#Orange.regression.linear.LinearRegressionLearner(preprocessors=None, fit_intercept=True)\n\n#Orange.regression.linear.RidgeRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=\'auto\', preprocessors=None)\n\n#Orange.regression.linear.LassoRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, preprocessors=None)\n', 'filename': 'C:/Users/00613/Sync-Austausch/_ML/_Orange/Orange-Python-Scripts/PolyFeatures.py'}], 'scriptText': '# Polynomial Regression\n########################################################\n# Settings:\nname="5"         # Name of the learner/model in other widgets, e.g. Test and Score\ndegree = 5            # maximal degree of the polynomial features\nfit_intercept = True  # \ntype = "0"            # 0: ordinary Least square \n                      # 1: Lasso (L1-Regularization)\n                      # 2: Ridge (L2-Regularization)\nalpha = 0.0001        # parameter alpha for L1- and L2-Regularization only\n#################################################################\n# File: Poly_Regression.py\n\n"""\n* Widget input: \n    - (optional) data \n    - (optional) preprocessors on object-input\n\n* Widget output: \n    - learner on learner-output, if no data on input\n    - learner and trained model on classifier-output, if data on input\n\n* Creates a learner for polynomial regression, which can be used e.g. in Test and Score-Widget\n* Additionally creates a trained model, if training data is connectet to data-input, e.g. for Predictions-Widget\n\n"""\n\nimport Orange\ndata = in_data\npreprocessors=in_objects\n\nif type == "0":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LinearRegressionLearner(fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: ordinary Least square")\n\nelif type == "1":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LassoRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept, max_iter=2000),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Lasso (L1)")\n\nelif type == "2":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.RidgeRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Ridge (L2)")\nelse:\n    print("no algorism specified")\n\nlearner.name = name\n\nif data != None:\n    classifier = learner(data)  \n    classifier.name = name\n\nout_learner = learner\nif data != None:\n    out_classifier = classifier \nelse:\n    out_classifier = None\nprint("\\nPreprocessors:")\nprint(preprocessors)\n\n\n# all possible settings for underlying learners for linear regression:\n#Orange.regression.linear.LinearRegressionLearner(preprocessors=None, fit_intercept=True)\n\n#Orange.regression.linear.RidgeRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=\'auto\', preprocessors=None)\n\n#Orange.regression.linear.LassoRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, preprocessors=None)\n', 'splitterState': b'\x00\x00\x00\xff\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x02\xbd\x00\x00\x00\xce\x01\xff\xff\xff\xff\x01\x00\x00\x00\x02\x00', 'vimModeEnabled': False, '__version__': 2}</properties>
		<properties node_id="9" format="literal">{'controlAreaVisible': True, 'currentScriptIndex': 0, 'savedWidgetGeometry': b'\x01\xd9\xd0\xcb\x00\x03\x00\x00\x00\x00\x00w\x00\x00\x00\x0f\x00\x00\x05w\x00\x00\x03\xe1\x00\x00\x00x\x00\x00\x00.\x00\x00\x05v\x00\x00\x03\xe0\x00\x00\x00\x00\x00\x00\x00\x00\x07\x80\x00\x00\x00x\x00\x00\x00.\x00\x00\x05v\x00\x00\x03\xe0', 'scriptLibrary': [{'name': 'Poly_Regression.py', 'script': '# Polynomial Regression\n########################################################\n# Settings:\nname="1"         # Name of the learner/model in other widgets, e.g. Test and Score\ndegree = 1            # maximal degree of the polynomial features\nfit_intercept = True  # \ntype = "0"            # 0: ordinary Least square \n                      # 1: Lasso (L1-Regularization)\n                      # 2: Ridge (L2-Regularization)\nalpha = 0.0001        # parameter alpha for L1- and L2-Regularization only\n#################################################################\n# File: Poly_Regression.py\n\n"""\n* Widget input: \n    - (optional) data \n    - (optional) preprocessors on object-input\n\n* Widget output: \n    - learner on learner-output, if no data on input\n    - learner and trained model on classifier-output, if data on input\n\n* Creates a learner for polynomial regression, which can be used e.g. in Test and Score-Widget\n* Additionally creates a trained model, if training data is connectet to data-input, e.g. for Predictions-Widget\n\n"""\n\nimport Orange\ndata = in_data\npreprocessors=in_objects\n\nif type == "0":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LinearRegressionLearner(fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: ordinary Least square")\n\nelif type == "1":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LassoRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept, max_iter=2000),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Lasso (L1)")\n\nelif type == "2":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.RidgeRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Ridge (L2)")\nelse:\n    print("no algorism specified")\n\nlearner.name = name\n\nif data != None:\n    classifier = learner(data)  \n    classifier.name = name\n\nout_learner = learner\nif data != None:\n    out_classifier = classifier \nelse:\n    out_classifier = None\nprint("\\nPreprocessors:")\nprint(preprocessors)\n\n\n# all possible settings for underlying learners for linear regression:\n#Orange.regression.linear.LinearRegressionLearner(preprocessors=None, fit_intercept=True)\n\n#Orange.regression.linear.RidgeRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=\'auto\', preprocessors=None)\n\n#Orange.regression.linear.LassoRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, preprocessors=None)\n', 'filename': 'C:/Users/00613/Sync-Austausch/_ML/_Orange/Orange-Python-Scripts/PolyFeatures.py'}], 'scriptText': '# Polynomial Regression\n########################################################\n# Settings:\nname="7"         # Name of the learner/model in other widgets, e.g. Test and Score\ndegree = 7            # maximal degree of the polynomial features\nfit_intercept = True  # \ntype = "0"            # 0: ordinary Least square \n                      # 1: Lasso (L1-Regularization)\n                      # 2: Ridge (L2-Regularization)\nalpha = 0.0001        # parameter alpha for L1- and L2-Regularization only\n#################################################################\n# File: Poly_Regression.py\n\n"""\n* Widget input: \n    - (optional) data \n    - (optional) preprocessors on object-input\n\n* Widget output: \n    - learner on learner-output, if no data on input\n    - learner and trained model on classifier-output, if data on input\n\n* Creates a learner for polynomial regression, which can be used e.g. in Test and Score-Widget\n* Additionally creates a trained model, if training data is connectet to data-input, e.g. for Predictions-Widget\n\n"""\n\nimport Orange\ndata = in_data\npreprocessors=in_objects\n\nif type == "0":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LinearRegressionLearner(fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: ordinary Least square")\n\nelif type == "1":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LassoRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept, max_iter=2000),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Lasso (L1)")\n\nelif type == "2":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.RidgeRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Ridge (L2)")\nelse:\n    print("no algorism specified")\n\nlearner.name = name\n\nif data != None:\n    classifier = learner(data)  \n    classifier.name = name\n\nout_learner = learner\nif data != None:\n    out_classifier = classifier \nelse:\n    out_classifier = None\nprint("\\nPreprocessors:")\nprint(preprocessors)\n\n\n# all possible settings for underlying learners for linear regression:\n#Orange.regression.linear.LinearRegressionLearner(preprocessors=None, fit_intercept=True)\n\n#Orange.regression.linear.RidgeRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=\'auto\', preprocessors=None)\n\n#Orange.regression.linear.LassoRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, preprocessors=None)\n', 'splitterState': b'\x00\x00\x00\xff\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x02\xbd\x00\x00\x00\xce\x01\xff\xff\xff\xff\x01\x00\x00\x00\x02\x00', 'vimModeEnabled': False, '__version__': 2}</properties>
		<properties node_id="10" format="literal">{'controlAreaVisible': True, 'currentScriptIndex': 0, 'savedWidgetGeometry': b'\x01\xd9\xd0\xcb\x00\x03\x00\x00\x00\x00\x00w\x00\x00\x00\x0f\x00\x00\x05w\x00\x00\x03\xe1\x00\x00\x00x\x00\x00\x00.\x00\x00\x05v\x00\x00\x03\xe0\x00\x00\x00\x00\x00\x00\x00\x00\x07\x80\x00\x00\x00x\x00\x00\x00.\x00\x00\x05v\x00\x00\x03\xe0', 'scriptLibrary': [{'name': 'Poly_Regression.py', 'script': '# Polynomial Regression\n########################################################\n# Settings:\nname="1"         # Name of the learner/model in other widgets, e.g. Test and Score\ndegree = 1            # maximal degree of the polynomial features\nfit_intercept = True  # \ntype = "0"            # 0: ordinary Least square \n                      # 1: Lasso (L1-Regularization)\n                      # 2: Ridge (L2-Regularization)\nalpha = 0.0001        # parameter alpha for L1- and L2-Regularization only\n#################################################################\n# File: Poly_Regression.py\n\n"""\n* Widget input: \n    - (optional) data \n    - (optional) preprocessors on object-input\n\n* Widget output: \n    - learner on learner-output, if no data on input\n    - learner and trained model on classifier-output, if data on input\n\n* Creates a learner for polynomial regression, which can be used e.g. in Test and Score-Widget\n* Additionally creates a trained model, if training data is connectet to data-input, e.g. for Predictions-Widget\n\n"""\n\nimport Orange\ndata = in_data\npreprocessors=in_objects\n\nif type == "0":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LinearRegressionLearner(fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: ordinary Least square")\n\nelif type == "1":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LassoRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept, max_iter=2000),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Lasso (L1)")\n\nelif type == "2":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.RidgeRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Ridge (L2)")\nelse:\n    print("no algorism specified")\n\nlearner.name = name\n\nif data != None:\n    classifier = learner(data)  \n    classifier.name = name\n\nout_learner = learner\nif data != None:\n    out_classifier = classifier \nelse:\n    out_classifier = None\nprint("\\nPreprocessors:")\nprint(preprocessors)\n\n\n# all possible settings for underlying learners for linear regression:\n#Orange.regression.linear.LinearRegressionLearner(preprocessors=None, fit_intercept=True)\n\n#Orange.regression.linear.RidgeRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=\'auto\', preprocessors=None)\n\n#Orange.regression.linear.LassoRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, preprocessors=None)\n', 'filename': 'C:/Users/00613/Sync-Austausch/_ML/_Orange/Orange-Python-Scripts/PolyFeatures.py'}], 'scriptText': '# Polynomial Regression\n########################################################\n# Settings:\nname="8"         # Name of the learner/model in other widgets, e.g. Test and Score\ndegree = 8            # maximal degree of the polynomial features\nfit_intercept = True  # \ntype = "0"            # 0: ordinary Least square \n                      # 1: Lasso (L1-Regularization)\n                      # 2: Ridge (L2-Regularization)\nalpha = 0.0001        # parameter alpha for L1- and L2-Regularization only\n#################################################################\n# File: Poly_Regression.py\n\n"""\n* Widget input: \n    - (optional) data \n    - (optional) preprocessors on object-input\n\n* Widget output: \n    - learner on learner-output, if no data on input\n    - learner and trained model on classifier-output, if data on input\n\n* Creates a learner for polynomial regression, which can be used e.g. in Test and Score-Widget\n* Additionally creates a trained model, if training data is connectet to data-input, e.g. for Predictions-Widget\n\n"""\n\nimport Orange\ndata = in_data\npreprocessors=in_objects\n\nif type == "0":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LinearRegressionLearner(fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: ordinary Least square")\n\nelif type == "1":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LassoRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept, max_iter=2000),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Lasso (L1)")\n\nelif type == "2":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.RidgeRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorism: Ridge (L2)")\nelse:\n    print("no algorism specified")\n\nlearner.name = name\n\nif data != None:\n    classifier = learner(data)  \n    classifier.name = name\n\nout_learner = learner\nif data != None:\n    out_classifier = classifier \nelse:\n    out_classifier = None\nprint("\\nPreprocessors:")\nprint(preprocessors)\n\n\n# all possible settings for underlying learners for linear regression:\n#Orange.regression.linear.LinearRegressionLearner(preprocessors=None, fit_intercept=True)\n\n#Orange.regression.linear.RidgeRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=\'auto\', preprocessors=None)\n\n#Orange.regression.linear.LassoRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, preprocessors=None)\n', 'splitterState': b'\x00\x00\x00\xff\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x02\xbd\x00\x00\x00\xce\x01\xff\xff\xff\xff\x01\x00\x00\x00\x02\x00', 'vimModeEnabled': False, '__version__': 2}</properties>
		<properties node_id="11" format="literal">{'controlAreaVisible': True, 'currentScriptIndex': 0, 'savedWidgetGeometry': b'\x01\xd9\xd0\xcb\x00\x03\x00\x00\x00\x00\x01b\x00\x00\x001\x00\x00\x06l\x00\x00\x03\xc7\x00\x00\x01c\x00\x00\x00P\x00\x00\x06k\x00\x00\x03\xc6\x00\x00\x00\x00\x00\x00\x00\x00\x07\x80\x00\x00\x01c\x00\x00\x00P\x00\x00\x06k\x00\x00\x03\xc6', 'scriptLibrary': [{'name': 'Validation_curve.py', 'script': '# Plot Validation Curve\n#######################\n# Settings:\nk = 5                       # number of folds\nproblem = "reg"             # "class": classification or "reg": Regression\nscore = "R2"                # "R2" or "MSE"; only for regression\nsave_results = False        # True: save Excelfile with results; False: don\'t save\nfile_path = "E:/Downloads/" # file path for save_results\n#                             e.g. "E:/Downloads/" - with slash (!) also in Windows (and trailing slash)\nreplicable = False          # WARNING: if using replicable=True, make sure to use shuffled data!\n#####################################################\n# File: Validation_curve_dev.py\n\n"""\n* Widget input: data and some learners\n* Widget output: -\n\n* Computes and shows validation curve for connected learners out of cross validation.\n* Uses standard error to indicate variability of the results.\n* Scores are CA (classification accuracy) for classification and R_squared (R2) or RMSE for regression.\n* Uses names of connected learners as x-labels.\n\nUsage: \n* mainly to compare different learner settings for manual hyper parameter optimization.\n* e.g. several learners of the same type, but with different values of a certain hyper parameter\n* Use name in learner widget to indicate different learners\n\n"""\n\nimport numpy as np\nfrom Orange.data import Table\nfrom Orange.evaluation import TestOnTestData\nimport Orange\nfrom sklearn import metrics\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\ndata = in_data.copy()\nlearners = in_learners\nif replicable == False: data.shuffle()\nn = len(data)\nX = data.X    # numpy.arrays\nY = data.Y    # numpy.arrays\n\nca = np.zeros([len(learners),k])\nmse= np.zeros([len(learners),k])\nr2 = np.zeros([len(learners),k])\nca_train = np.zeros([len(learners),k])\nmse_train = np.zeros([len(learners),k])\nr2_train = np.zeros([len(learners),k])\n\nfor fold in range(k):\n    # Prepairing Folds\n    #-----------------\n    x_train = np.vstack((X[0:fold*int(n/k)], X[(fold+1)*int(n/k):]))\n    if fold==0:\n        y_train = Y[(fold+1)*int(n/k):]\n    else:\n        y_train = np.append(Y[0:fold*int(n/k)], Y[(fold+1)*int(n/k):])\n    x_test  = X[fold*int(n/k): (fold+1)*int(n/k)]\n    y_test  = Y[fold*int(n/k): (fold+1)*int(n/k)]\n\n    trainset = Table.from_numpy(data.domain, X=x_train, Y=y_train)\n    testset = Table.from_numpy(data.domain, X=x_test, Y=y_test)\n    \n    # Training models on Folds and make predictions\n    #----------------------------------------------\n    models = [learner(trainset) for learner in learners]\n    y_pred_test = [model(testset) for model in models]\n    y_pred_train = [model(trainset) for model in models]\n    \n    # Calculate metrics\n    #------------------\n    if problem == "class":\n        for learner,_ in enumerate(learners):\n            # Metrics see https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n            # use only CA to avoid problems with multiclass labels\n            ca[learner, fold] = metrics.accuracy_score(y_test, y_pred_test[learner])\n            ca_train[learner, fold] = metrics.accuracy_score(y_train, y_pred_train[learner])\n            \n    if problem == "reg":\n        for learner,_ in enumerate(learners):\n            # other metrics would be possible; these two should be sufficient\n            mse[learner,fold] = metrics.mean_squared_error(y_test, y_pred_test[learner])\n            r2[learner,fold] = metrics.r2_score(y_test, y_pred_test[learner])\n            mse_train[learner, fold] = metrics.mean_squared_error(y_train, y_pred_train[learner])\n            r2_train[learner, fold] = metrics.r2_score(y_train, y_pred_train[learner])\n\n# Aggregate results (means and standard errors)\n#----------------------------------------------\nresult_table = pd.DataFrame([])\nresult_table["Models"] = [F"{learners[learner]}" for learner,_ in enumerate(learners)]\nif problem == "class":\n    result_table["CA"] = [ca[learner].mean() for learner,_ in enumerate(learners)]\n    result_table["CA-se"] = [ca[learner].std(ddof=1)/k for learner,_ in enumerate(learners)]\n    result_table["CA_train"] = [ca_train[learner].mean() for learner,_ in enumerate(learners)]\n    result_table["CA-se_train"] = [ca_train[learner].std(ddof=1)/k for learner,_ in enumerate(learners)]\n    \nif problem == "reg":\n    result_table["MSE"] = [mse[learner].mean() for learner,_ in enumerate(learners)]\n    result_table["MSE-se"] = [mse[learner].std(ddof=1)/k for learner,_ in enumerate(learners)]\n    result_table["R2"] = [r2[learner].mean() for learner,_ in enumerate(learners)]\n    result_table["R2-se"] = [r2[learner].std(ddof=1)/k for learner,_ in enumerate(learners)]\n    result_table["MSE_train"] = [mse_train[learner].mean() for learner,_ in enumerate(learners)]\n    result_table["MSE-se_train"] = [mse_train[learner].std(ddof=1)/k for learner,_ in enumerate(learners)]\n    result_table["R2_train"] = [r2_train[learner].mean() for learner,_ in enumerate(learners)]\n    result_table["R2-se_train"] = [r2_train[learner].std(ddof=1)/k for learner,_ in enumerate(learners)]\n\n# Print to table and save data\n#-----------------------------\nprint("---------------------------")\nprint("Results of Cross validation")\nprint("---------------------------")\nprint("Number of instances: ", len(data))\nprint("Number of folds    : ", k)\nprint("")\nif problem == "class":\n    print(result_table)\nif problem == "reg":\n    print(result_table[["Models", "MSE", "MSE-se", "R2", "R2-se"]])\n    print(result_table[["Models", "MSE_train", "MSE-se_train", "R2_train", "R2-se_train"]])\n\nif save_results:\n    file_name_path = file_path + "Results-" + datetime.now().strftime("%Y-%m-%d_%H-%M-%S") + ".xlsx"\n    print("Results in: ",file_name_path)\n    with pd.ExcelWriter(file_name_path) as writer:\n        result_table.to_excel(writer)\n\n# Create Diagram\n#---------------\nfig = plt.figure(figsize=(8,6))\nplt.title("Validation Curve")\nlw = 2\nif problem == "class":\n    plt.ylabel("CA")\n    plt.plot(\n        result_table["Models"].values, result_table["CA_train"].values, label="Training score", color="darkorange", lw=lw\n    )\n    plt.fill_between(\n        result_table["Models"].values,\n        result_table["CA_train"].values - result_table["CA-se_train"].values,\n        result_table["CA_train"].values + result_table["CA-se_train"].values,\n        alpha=0.2,\n        color="darkorange",\n        lw=lw,\n    )\n    plt.plot(\n        result_table["Models"].values, result_table["CA"].values, label="Cross-validation score", color="navy"\n    )\n    plt.fill_between(\n        result_table["Models"].values,\n        result_table["CA"].values - result_table["CA-se"].values,\n        result_table["CA"].values + result_table["CA-se"].values,\n        alpha=0.2,\n        color="navy",\n        lw=lw,\n    )\n    plt.legend(loc="best")\n    plt.show()\n    \nif problem == "reg":\n    if score == "MSE":\n        plt.ylabel("MSE")\n        plt.plot(\n            result_table["Models"].values, result_table["MSE_train"].values, label="Training score", color="darkorange", lw=lw\n        )\n        plt.fill_between(\n            result_table["Models"].values,\n            result_table["MSE_train"].values - result_table["MSE-se_train"].values,\n            result_table["MSE_train"].values + result_table["MSE-se_train"].values,\n            alpha=0.2,\n            color="darkorange",\n            lw=lw,\n        )\n        plt.plot(\n            result_table["Models"].values, result_table["MSE"].values, label="Cross-validation score", color="navy"\n        )\n        plt.fill_between(\n            result_table["Models"].values,\n            result_table["MSE"].values - result_table["MSE-se"].values,\n            result_table["MSE"].values + result_table["MSE-se"].values,\n            alpha=0.2,\n            color="navy",\n            lw=lw,\n        )\n        plt.legend(loc="best")\n        plt.show()\n    else:\n        plt.ylabel("R2")\n        plt.plot(\n            result_table["Models"].values, result_table["R2_train"].values, label="Training score", color="darkorange", lw=lw\n        )\n        plt.fill_between(\n            result_table["Models"].values,\n            result_table["R2_train"].values - result_table["R2-se_train"].values,\n            result_table["R2_train"].values + result_table["R2-se_train"].values,\n            alpha=0.2,\n            color="darkorange",\n            lw=lw,\n        )\n        plt.plot(\n            result_table["Models"].values, result_table["R2"].values, label="Cross-validation score", color="navy"\n        )\n        plt.fill_between(\n            result_table["Models"].values,\n            result_table["R2"].values - result_table["R2-se"].values,\n            result_table["R2"].values + result_table["R2-se"].values,\n            alpha=0.2,\n            color="navy",\n            lw=lw,\n        )\n        plt.legend(loc="best")\n        plt.show()\n\n\n', 'filename': 'C:/Users/00613/Sync-Austausch/_ML/_Orange/Orange-Python-Scripts/Validation_curve.py'}], 'scriptText': '# Plot Validation Curve\n#######################\n# Settings:\nk = 5                       # number of folds\nproblem = "reg"             # "class": classification or "reg": Regression\nscore = "R2"                # "R2" or "MSE"; only for regression\nsave_results = False        # True: save Excelfile with results; False: don\'t save\nfile_path = "E:/Downloads/" # file path for save_results\n#                             e.g. "E:/Downloads/" - with slash (!) also in Windows (and trailing slash)\nreplicable = False          # WARNING: if using replicable=True, make sure to use shuffled data!\n#####################################################\n# File: Validation_curve_dev.py\n\n"""\n* Widget input: data and some learners\n* Widget output: -\n\n* Computes and shows validation curve for connected learners out of cross validation.\n* Uses standard error to indicate variability of the results.\n* Scores are CA (classification accuracy) for classification and R_squared (R2) or RMSE for regression.\n* Uses names of connected learners as x-labels.\n\nUsage: \n* mainly to compare different learner settings for manual hyper parameter optimization.\n* e.g. several learners of the same type, but with different values of a certain hyper parameter\n* Use name in learner widget to indicate different learners\n\n"""\n\nimport numpy as np\nfrom Orange.data import Table\nfrom Orange.evaluation import TestOnTestData\nimport Orange\nfrom sklearn import metrics\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\ndata = in_data.copy()\nlearners = in_learners\nif replicable == False: data.shuffle()\nn = len(data)\nX = data.X    # numpy.arrays\nY = data.Y    # numpy.arrays\n\nca = np.zeros([len(learners),k])\nmse= np.zeros([len(learners),k])\nr2 = np.zeros([len(learners),k])\nca_train = np.zeros([len(learners),k])\nmse_train = np.zeros([len(learners),k])\nr2_train = np.zeros([len(learners),k])\n\nfor fold in range(k):\n    # Prepairing Folds\n    #-----------------\n    x_train = np.vstack((X[0:fold*int(n/k)], X[(fold+1)*int(n/k):]))\n    if fold==0:\n        y_train = Y[(fold+1)*int(n/k):]\n    else:\n        y_train = np.append(Y[0:fold*int(n/k)], Y[(fold+1)*int(n/k):])\n    x_test  = X[fold*int(n/k): (fold+1)*int(n/k)]\n    y_test  = Y[fold*int(n/k): (fold+1)*int(n/k)]\n\n    trainset = Table.from_numpy(data.domain, X=x_train, Y=y_train)\n    testset = Table.from_numpy(data.domain, X=x_test, Y=y_test)\n    \n    # Training models on Folds and make predictions\n    #----------------------------------------------\n    models = [learner(trainset) for learner in learners]\n    y_pred_test = [model(testset) for model in models]\n    y_pred_train = [model(trainset) for model in models]\n    \n    # Calculate metrics\n    #------------------\n    if problem == "class":\n        for learner,_ in enumerate(learners):\n            # Metrics see https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n            # use only CA to avoid problems with multiclass labels\n            ca[learner, fold] = metrics.accuracy_score(y_test, y_pred_test[learner])\n            ca_train[learner, fold] = metrics.accuracy_score(y_train, y_pred_train[learner])\n            \n    if problem == "reg":\n        for learner,_ in enumerate(learners):\n            # other metrics would be possible; these two should be sufficient\n            mse[learner,fold] = metrics.mean_squared_error(y_test, y_pred_test[learner])\n            r2[learner,fold] = metrics.r2_score(y_test, y_pred_test[learner])\n            mse_train[learner, fold] = metrics.mean_squared_error(y_train, y_pred_train[learner])\n            r2_train[learner, fold] = metrics.r2_score(y_train, y_pred_train[learner])\n\n# Aggregate results (means and standard errors)\n#----------------------------------------------\nresult_table = pd.DataFrame([])\nresult_table["Models"] = [F"{learners[learner]}" for learner,_ in enumerate(learners)]\nif problem == "class":\n    result_table["CA"] = [ca[learner].mean() for learner,_ in enumerate(learners)]\n    result_table["CA-se"] = [ca[learner].std(ddof=1)/k for learner,_ in enumerate(learners)]\n    result_table["CA_train"] = [ca_train[learner].mean() for learner,_ in enumerate(learners)]\n    result_table["CA-se_train"] = [ca_train[learner].std(ddof=1)/k for learner,_ in enumerate(learners)]\n    \nif problem == "reg":\n    result_table["MSE"] = [mse[learner].mean() for learner,_ in enumerate(learners)]\n    result_table["MSE-se"] = [mse[learner].std(ddof=1)/k for learner,_ in enumerate(learners)]\n    result_table["R2"] = [r2[learner].mean() for learner,_ in enumerate(learners)]\n    result_table["R2-se"] = [r2[learner].std(ddof=1)/k for learner,_ in enumerate(learners)]\n    result_table["MSE_train"] = [mse_train[learner].mean() for learner,_ in enumerate(learners)]\n    result_table["MSE-se_train"] = [mse_train[learner].std(ddof=1)/k for learner,_ in enumerate(learners)]\n    result_table["R2_train"] = [r2_train[learner].mean() for learner,_ in enumerate(learners)]\n    result_table["R2-se_train"] = [r2_train[learner].std(ddof=1)/k for learner,_ in enumerate(learners)]\n\n# Print to table and save data\n#-----------------------------\nprint("---------------------------")\nprint("Results of Cross validation")\nprint("---------------------------")\nprint("Number of instances: ", len(data))\nprint("Number of folds    : ", k)\nprint("")\nif problem == "class":\n    print(result_table)\nif problem == "reg":\n    print(result_table[["Models", "MSE", "MSE-se", "R2", "R2-se"]])\n    print(result_table[["Models", "MSE_train", "MSE-se_train", "R2_train", "R2-se_train"]])\n\nif save_results:\n    file_name_path = file_path + "Results-" + datetime.now().strftime("%Y-%m-%d_%H-%M-%S") + ".xlsx"\n    print("Results in: ",file_name_path)\n    with pd.ExcelWriter(file_name_path) as writer:\n        result_table.to_excel(writer)\n\n# Create Diagram\n#---------------\nfig = plt.figure(figsize=(8,6))\nplt.title("Validation Curve")\nlw = 2\nif problem == "class":\n    plt.ylabel("CA")\n    plt.plot(\n        result_table["Models"].values, result_table["CA_train"].values, label="Training score", color="darkorange", lw=lw\n    )\n    plt.fill_between(\n        result_table["Models"].values,\n        result_table["CA_train"].values - result_table["CA-se_train"].values,\n        result_table["CA_train"].values + result_table["CA-se_train"].values,\n        alpha=0.2,\n        color="darkorange",\n        lw=lw,\n    )\n    plt.plot(\n        result_table["Models"].values, result_table["CA"].values, label="Cross-validation score", color="navy"\n    )\n    plt.fill_between(\n        result_table["Models"].values,\n        result_table["CA"].values - result_table["CA-se"].values,\n        result_table["CA"].values + result_table["CA-se"].values,\n        alpha=0.2,\n        color="navy",\n        lw=lw,\n    )\n    plt.legend(loc="best")\n    plt.show()\n    \nif problem == "reg":\n    if score == "MSE":\n        plt.ylabel("MSE")\n        plt.plot(\n            result_table["Models"].values, result_table["MSE_train"].values, label="Training score", color="darkorange", lw=lw\n        )\n        plt.fill_between(\n            result_table["Models"].values,\n            result_table["MSE_train"].values - result_table["MSE-se_train"].values,\n            result_table["MSE_train"].values + result_table["MSE-se_train"].values,\n            alpha=0.2,\n            color="darkorange",\n            lw=lw,\n        )\n        plt.plot(\n            result_table["Models"].values, result_table["MSE"].values, label="Cross-validation score", color="navy"\n        )\n        plt.fill_between(\n            result_table["Models"].values,\n            result_table["MSE"].values - result_table["MSE-se"].values,\n            result_table["MSE"].values + result_table["MSE-se"].values,\n            alpha=0.2,\n            color="navy",\n            lw=lw,\n        )\n        plt.legend(loc="best")\n        plt.show()\n    else:\n        plt.ylabel("R2")\n        plt.plot(\n            result_table["Models"].values, result_table["R2_train"].values, label="Training score", color="darkorange", lw=lw\n        )\n        plt.fill_between(\n            result_table["Models"].values,\n            result_table["R2_train"].values - result_table["R2-se_train"].values,\n            result_table["R2_train"].values + result_table["R2-se_train"].values,\n            alpha=0.2,\n            color="darkorange",\n            lw=lw,\n        )\n        plt.plot(\n            result_table["Models"].values, result_table["R2"].values, label="Cross-validation score", color="navy"\n        )\n        plt.fill_between(\n            result_table["Models"].values,\n            result_table["R2"].values - result_table["R2-se"].values,\n            result_table["R2"].values + result_table["R2-se"].values,\n            alpha=0.2,\n            color="navy",\n            lw=lw,\n        )\n        plt.legend(loc="best")\n        plt.show()\n\n\n', 'splitterState': b'\x00\x00\x00\xff\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x01\xe5\x00\x00\x01j\x01\xff\xff\xff\xff\x01\x00\x00\x00\x02\x00', 'vimModeEnabled': False, '__version__': 2}</properties>
	</node_properties>
	<session_state>
		<window_groups />
	</session_state>
</scheme>
