<?xml version='1.0' encoding='utf-8'?>
<scheme version="2.0" title="Some Python-Scripts to add fuctionality to Orange-Flows" description="Some Python-Scripts to add fuctionality to Orange-Flows">
	<nodes>
		<node id="0" name="Python Script" qualified_name="Orange.widgets.data.owpythonscript.OWPythonScript" project_name="Orange3" version="" title="Validation_curve_old.py" position="(595.0, 453.0)" />
		<node id="1" name="Python Script" qualified_name="Orange.widgets.data.owpythonscript.OWPythonScript" project_name="Orange3" version="" title="Diagram_target_predictions2D.py" position="(300, 300)" />
		<node id="2" name="Python Script" qualified_name="Orange.widgets.data.owpythonscript.OWPythonScript" project_name="Orange3" version="" title="Diagram_partition_boundaries.py" position="(300, 450)" />
		<node id="3" name="Python Script" qualified_name="Orange.widgets.data.owpythonscript.OWPythonScript" project_name="Orange3" version="" title="Plot_Scatter_Matrix.py" position="(150, 450)" />
		<node id="4" name="Python Script" qualified_name="Orange.widgets.data.owpythonscript.OWPythonScript" project_name="Orange3" version="" title="PolyFeatures.py" position="(450, 150)" />
		<node id="5" name="Python Script" qualified_name="Orange.widgets.data.owpythonscript.OWPythonScript" project_name="Orange3" version="" title="Display_MNIST-Image.py" position="(447.0, 451.0)" />
		<node id="6" name="Python Script" qualified_name="Orange.widgets.data.owpythonscript.OWPythonScript" project_name="Orange3" version="" title="Poly_Regression.py" position="(450.0, 300.0)" />
		<node id="7" name="Python Script" qualified_name="Orange.widgets.data.owpythonscript.OWPythonScript" project_name="Orange3" version="" title="Evaluation_Results_binary.py" position="(596.0, 300.0)" />
		<node id="8" name="Python Script" qualified_name="Orange.widgets.data.owpythonscript.OWPythonScript" project_name="Orange3" version="" title="Learning_curve.py" position="(151.0, 301.0)" />
		<node id="9" name="Python Script" qualified_name="Orange.widgets.data.owpythonscript.OWPythonScript" project_name="Orange3" version="" title="Diagram_target_predictions.py" position="(303.0, 151.0)" />
		<node id="10" name="Python Script" qualified_name="Orange.widgets.data.owpythonscript.OWPythonScript" project_name="Orange3" version="" title="Evaluation_results.py" position="(605.0, 150.0)" />
		<node id="11" name="Python Script" qualified_name="Orange.widgets.data.owpythonscript.OWPythonScript" project_name="Orange3" version="" title="Validation_curve.py" position="(152.0, 152.0)" />
	</nodes>
	<links />
	<annotations>
		<text id="0" type="text/markdown" rect="(85.0, 29.0, 486.0, 82.0)" font-family="MS Shell Dlg 2" font-size="16"># Orange-Support-Files

__Some Python-Scripts to add functionality to Orange-Flows__</text>
		<text id="1" type="text/rst" rect="(83.0, 554.0, 575.0, 202.0)" font-family="MS Shell Dlg 2" font-size="16">version 0.2.4

Documentation:

- see short explanation in scripts

- see documentation in

https://github.com/EKal-aa/orange-support-files/blob/main/doc/readme.md</text>
		<text id="2" type="text/plain" rect="(554.0, 359.0, 90.0, 31.0)" font-family="MS Shell Dlg 2" font-size="16">deprecated</text>
	</annotations>
	<thumbnail />
	<node_properties>
		<properties node_id="0" format="literal">{'controlAreaVisible': True, 'currentScriptIndex': 0, 'savedWidgetGeometry': b'\x01\xd9\xd0\xcb\x00\x03\x00\x00\x00\x00\x01D\x00\x00\x00H\x00\x00\x06N\x00\x00\x03\xde\x00\x00\x01E\x00\x00\x00g\x00\x00\x06M\x00\x00\x03\xdd\x00\x00\x00\x00\x00\x00\x00\x00\x07\x80\x00\x00\x01E\x00\x00\x00g\x00\x00\x06M\x00\x00\x03\xdd', 'scriptLibrary': [{'name': 'Validation_curve.py', 'script': '# Plot Validation Curve\n#######################\n# Settings:\nk = 5                       # number of folds\nproblem = "reg"             # "class": classification or "reg": Regression\nscore = "R2"                # "R2" or "MSE"; only for regression\nsave_results = False        # True: save Excelfile with results; False: don\'t save\nfile_path = "E:/Downloads/" # file path for save_results\n#                             e.g. "E:/Downloads/" - with slash (!) also in Windows (and trailing slash)\nreplicable = False          # WARNING: if using replicable=True, make sure to use shuffled data!\n#####################################################\n# File: Validation_curve_dev.py\n\n"""\n* Widget input: data and some learners\n* Widget output: -\n\n* Computes and shows validation curve for connected learners out of cross validation.\n* Uses standard error to indicate variability of the results.\n* Scores are CA (classification accuracy) for classification and R_squared (R2) or RMSE for regression.\n* Uses names of connected learners as x-labels.\n\nUsage: \n* mainly to compare different learner settings for manual hyper parameter optimization.\n* e.g. several learners of the same type, but with different values of a certain hyper parameter\n* Use name in learner widget to indicate different learners\n\n"""\n\nimport numpy as np\nfrom Orange.data import Table\nfrom Orange.evaluation import TestOnTestData\nimport Orange\nfrom sklearn import metrics\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\ndata = in_data.copy()\nlearners = in_learners\nif replicable == False: data.shuffle()\nn = len(data)\nX = data.X    # numpy.arrays\nY = data.Y    # numpy.arrays\n\nca = np.zeros([len(learners),k])\nmse= np.zeros([len(learners),k])\nr2 = np.zeros([len(learners),k])\nca_train = np.zeros([len(learners),k])\nmse_train = np.zeros([len(learners),k])\nr2_train = np.zeros([len(learners),k])\n\nfor fold in range(k):\n    # Prepairing Folds\n    #-----------------\n    x_train = np.vstack((X[0:fold*int(n/k)], X[(fold+1)*int(n/k):]))\n    if fold==0:\n        y_train = Y[(fold+1)*int(n/k):]\n    else:\n        y_train = np.append(Y[0:fold*int(n/k)], Y[(fold+1)*int(n/k):])\n    x_test  = X[fold*int(n/k): (fold+1)*int(n/k)]\n    y_test  = Y[fold*int(n/k): (fold+1)*int(n/k)]\n\n    trainset = Table.from_numpy(data.domain, X=x_train, Y=y_train)\n    testset = Table.from_numpy(data.domain, X=x_test, Y=y_test)\n    \n    # Training models on Folds and make predictions\n    #----------------------------------------------\n    models = [learner(trainset) for learner in learners]\n    y_pred_test = [model(testset) for model in models]\n    y_pred_train = [model(trainset) for model in models]\n    \n    # Calculate metrics\n    #------------------\n    if problem == "class":\n        for learner,_ in enumerate(learners):\n            # Metrics see https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n            # use only CA to avoid problems with multiclass labels\n            ca[learner, fold] = metrics.accuracy_score(y_test, y_pred_test[learner])\n            ca_train[learner, fold] = metrics.accuracy_score(y_train, y_pred_train[learner])\n            \n    if problem == "reg":\n        for learner,_ in enumerate(learners):\n            # other metrics would be possible; these two should be sufficient\n            mse[learner,fold] = metrics.mean_squared_error(y_test, y_pred_test[learner])\n            r2[learner,fold] = metrics.r2_score(y_test, y_pred_test[learner])\n            mse_train[learner, fold] = metrics.mean_squared_error(y_train, y_pred_train[learner])\n            r2_train[learner, fold] = metrics.r2_score(y_train, y_pred_train[learner])\n\n# Aggregate results (means and standard errors)\n#----------------------------------------------\nresult_table = pd.DataFrame([])\nresult_table["Models"] = [F"{learners[learner]}" for learner,_ in enumerate(learners)]\nif problem == "class":\n    result_table["CA"] = [ca[learner].mean() for learner,_ in enumerate(learners)]\n    result_table["CA-se"] = [ca[learner].std(ddof=1)/k for learner,_ in enumerate(learners)]\n    result_table["CA_train"] = [ca_train[learner].mean() for learner,_ in enumerate(learners)]\n    result_table["CA-se_train"] = [ca_train[learner].std(ddof=1)/k for learner,_ in enumerate(learners)]\n    \nif problem == "reg":\n    result_table["MSE"] = [mse[learner].mean() for learner,_ in enumerate(learners)]\n    result_table["MSE-se"] = [mse[learner].std(ddof=1)/k for learner,_ in enumerate(learners)]\n    result_table["R2"] = [r2[learner].mean() for learner,_ in enumerate(learners)]\n    result_table["R2-se"] = [r2[learner].std(ddof=1)/k for learner,_ in enumerate(learners)]\n    result_table["MSE_train"] = [mse_train[learner].mean() for learner,_ in enumerate(learners)]\n    result_table["MSE-se_train"] = [mse_train[learner].std(ddof=1)/k for learner,_ in enumerate(learners)]\n    result_table["R2_train"] = [r2_train[learner].mean() for learner,_ in enumerate(learners)]\n    result_table["R2-se_train"] = [r2_train[learner].std(ddof=1)/k for learner,_ in enumerate(learners)]\n\n# Print to table and save data\n#-----------------------------\nprint("---------------------------")\nprint("Results of Cross validation")\nprint("---------------------------")\nprint("Number of instances: ", len(data))\nprint("Number of folds    : ", k)\nprint("")\nif problem == "class":\n    print(result_table)\nif problem == "reg":\n    print(result_table[["Models", "MSE", "MSE-se", "R2", "R2-se"]])\n    print(result_table[["Models", "MSE_train", "MSE-se_train", "R2_train", "R2-se_train"]])\n\nif save_results:\n    file_name_path = file_path + "Results-" + datetime.now().strftime("%Y-%m-%d_%H-%M-%S") + ".xlsx"\n    print("Results in: ",file_name_path)\n    with pd.ExcelWriter(file_name_path) as writer:\n        result_table.to_excel(writer)\n\n# Create Diagram\n#---------------\nfig = plt.figure(figsize=(8,6))\nplt.title("Validation Curve")\nlw = 2\nif problem == "class":\n    plt.ylabel("CA")\n    plt.plot(\n        result_table["Models"].values, result_table["CA_train"].values, label="Training score", color="darkorange", lw=lw\n    )\n    plt.fill_between(\n        result_table["Models"].values,\n        result_table["CA_train"].values - result_table["CA-se_train"].values,\n        result_table["CA_train"].values + result_table["CA-se_train"].values,\n        alpha=0.2,\n        color="darkorange",\n        lw=lw,\n    )\n    plt.plot(\n        result_table["Models"].values, result_table["CA"].values, label="Cross-validation score", color="navy"\n    )\n    plt.fill_between(\n        result_table["Models"].values,\n        result_table["CA"].values - result_table["CA-se"].values,\n        result_table["CA"].values + result_table["CA-se"].values,\n        alpha=0.2,\n        color="navy",\n        lw=lw,\n    )\n    plt.legend(loc="best")\n    plt.show()\n    \nif problem == "reg":\n    if score == "MSE":\n        plt.ylabel("MSE")\n        plt.plot(\n            result_table["Models"].values, result_table["MSE_train"].values, label="Training score", color="darkorange", lw=lw\n        )\n        plt.fill_between(\n            result_table["Models"].values,\n            result_table["MSE_train"].values - result_table["MSE-se_train"].values,\n            result_table["MSE_train"].values + result_table["MSE-se_train"].values,\n            alpha=0.2,\n            color="darkorange",\n            lw=lw,\n        )\n        plt.plot(\n            result_table["Models"].values, result_table["MSE"].values, label="Cross-validation score", color="navy"\n        )\n        plt.fill_between(\n            result_table["Models"].values,\n            result_table["MSE"].values - result_table["MSE-se"].values,\n            result_table["MSE"].values + result_table["MSE-se"].values,\n            alpha=0.2,\n            color="navy",\n            lw=lw,\n        )\n        plt.legend(loc="best")\n        plt.show()\n    else:\n        plt.ylabel("R2")\n        plt.plot(\n            result_table["Models"].values, result_table["R2_train"].values, label="Training score", color="darkorange", lw=lw\n        )\n        plt.fill_between(\n            result_table["Models"].values,\n            result_table["R2_train"].values - result_table["R2-se_train"].values,\n            result_table["R2_train"].values + result_table["R2-se_train"].values,\n            alpha=0.2,\n            color="darkorange",\n            lw=lw,\n        )\n        plt.plot(\n            result_table["Models"].values, result_table["R2"].values, label="Cross-validation score", color="navy"\n        )\n        plt.fill_between(\n            result_table["Models"].values,\n            result_table["R2"].values - result_table["R2-se"].values,\n            result_table["R2"].values + result_table["R2-se"].values,\n            alpha=0.2,\n            color="navy",\n            lw=lw,\n        )\n        plt.legend(loc="best")\n        plt.show()\n\n\n', 'filename': 'C:/Users/00613/Sync-Austausch/_ML/_Orange/Orange-Python-Scripts/Validation_curve.py'}], 'scriptText': '# Plot Validation Curve\n#######################\n# Settings:\nk = 5                       # number of folds\nproblem = "reg"             # "class": classification or "reg": Regression\nscore = "R2"                # "R2" or "MSE"; only for regression\nsave_results = False        # True: save Excelfile with results; False: don\'t save\nfile_path = "E:/Downloads/" # file path for save_results\n#                             e.g. "E:/Downloads/" - with slash (!) also in Windows (and trailing slash)\nreplicable = False          # WARNING: if using replicable=True, make sure to use shuffled data!\n#####################################################\n# File: Validation_curve_dev.py\n\n"""\n* Widget input: data and some learners\n* Widget output: -\n\n* Computes and shows validation curve for connected learners out of cross validation.\n* Uses standard error to indicate variability of the results.\n* Scores are CA (classification accuracy) for classification and R_squared (R2) or RMSE for regression.\n* Uses names of connected learners as x-labels.\n\nUsage: \n* mainly to compare different learner settings for manual hyper parameter optimization.\n* e.g. several learners of the same type, but with different values of a certain hyper parameter\n* Use name in learner widget to indicate different learners\n\n"""\n\nimport numpy as np\nfrom Orange.data import Table\nfrom Orange.evaluation import TestOnTestData\nimport Orange\nfrom sklearn import metrics\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\ndata = in_data.copy()\nlearners = in_learners\nif replicable == False: data.shuffle()\nn = len(data)\nX = data.X    # numpy.arrays\nY = data.Y    # numpy.arrays\n\nca = np.zeros([len(learners),k])\nmse= np.zeros([len(learners),k])\nr2 = np.zeros([len(learners),k])\nca_train = np.zeros([len(learners),k])\nmse_train = np.zeros([len(learners),k])\nr2_train = np.zeros([len(learners),k])\n\nfor fold in range(k):\n    # Prepairing Folds\n    #-----------------\n    x_train = np.vstack((X[0:fold*int(n/k)], X[(fold+1)*int(n/k):]))\n    if fold==0:\n        y_train = Y[(fold+1)*int(n/k):]\n    else:\n        y_train = np.append(Y[0:fold*int(n/k)], Y[(fold+1)*int(n/k):])\n    x_test  = X[fold*int(n/k): (fold+1)*int(n/k)]\n    y_test  = Y[fold*int(n/k): (fold+1)*int(n/k)]\n\n    trainset = Table.from_numpy(data.domain, X=x_train, Y=y_train)\n    testset = Table.from_numpy(data.domain, X=x_test, Y=y_test)\n    \n    # Training models on Folds and make predictions\n    #----------------------------------------------\n    models = [learner(trainset) for learner in learners]\n    y_pred_test = [model(testset) for model in models]\n    y_pred_train = [model(trainset) for model in models]\n    \n    # Calculate metrics\n    #------------------\n    if problem == "class":\n        for learner,_ in enumerate(learners):\n            # Metrics see https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n            # use only CA to avoid problems with multiclass labels\n            ca[learner, fold] = metrics.accuracy_score(y_test, y_pred_test[learner])\n            ca_train[learner, fold] = metrics.accuracy_score(y_train, y_pred_train[learner])\n            \n    if problem == "reg":\n        for learner,_ in enumerate(learners):\n            # other metrics would be possible; these two should be sufficient\n            mse[learner,fold] = metrics.mean_squared_error(y_test, y_pred_test[learner])\n            r2[learner,fold] = metrics.r2_score(y_test, y_pred_test[learner])\n            mse_train[learner, fold] = metrics.mean_squared_error(y_train, y_pred_train[learner])\n            r2_train[learner, fold] = metrics.r2_score(y_train, y_pred_train[learner])\n\n# Aggregate results (means and standard errors)\n#----------------------------------------------\nresult_table = pd.DataFrame([])\nresult_table["Models"] = [F"{learners[learner]}" for learner,_ in enumerate(learners)]\nif problem == "class":\n    result_table["CA"] = [ca[learner].mean() for learner,_ in enumerate(learners)]\n    result_table["CA-se"] = [ca[learner].std(ddof=1)/k for learner,_ in enumerate(learners)]\n    result_table["CA_train"] = [ca_train[learner].mean() for learner,_ in enumerate(learners)]\n    result_table["CA-se_train"] = [ca_train[learner].std(ddof=1)/k for learner,_ in enumerate(learners)]\n    \nif problem == "reg":\n    result_table["MSE"] = [mse[learner].mean() for learner,_ in enumerate(learners)]\n    result_table["MSE-se"] = [mse[learner].std(ddof=1)/k for learner,_ in enumerate(learners)]\n    result_table["R2"] = [r2[learner].mean() for learner,_ in enumerate(learners)]\n    result_table["R2-se"] = [r2[learner].std(ddof=1)/k for learner,_ in enumerate(learners)]\n    result_table["MSE_train"] = [mse_train[learner].mean() for learner,_ in enumerate(learners)]\n    result_table["MSE-se_train"] = [mse_train[learner].std(ddof=1)/k for learner,_ in enumerate(learners)]\n    result_table["R2_train"] = [r2_train[learner].mean() for learner,_ in enumerate(learners)]\n    result_table["R2-se_train"] = [r2_train[learner].std(ddof=1)/k for learner,_ in enumerate(learners)]\n\n# Print to table and save data\n#-----------------------------\nprint("---------------------------")\nprint("Results of Cross validation")\nprint("---------------------------")\nprint("Number of instances: ", len(data))\nprint("Number of folds    : ", k)\nprint("")\nif problem == "class":\n    print(result_table)\nif problem == "reg":\n    print(result_table[["Models", "MSE", "MSE-se", "R2", "R2-se"]])\n    print(result_table[["Models", "MSE_train", "MSE-se_train", "R2_train", "R2-se_train"]])\n\nif save_results:\n    file_name_path = file_path + "Results-" + datetime.now().strftime("%Y-%m-%d_%H-%M-%S") + ".xlsx"\n    print("Results in: ",file_name_path)\n    with pd.ExcelWriter(file_name_path) as writer:\n        result_table.to_excel(writer)\n\n# Create Diagram\n#---------------\nfig = plt.figure(figsize=(8,6))\nplt.title("Validation Curve")\nlw = 2\nif problem == "class":\n    plt.ylabel("CA")\n    plt.plot(\n        result_table["Models"].values, result_table["CA_train"].values, label="Training score", color="darkorange", lw=lw\n    )\n    plt.fill_between(\n        result_table["Models"].values,\n        result_table["CA_train"].values - result_table["CA-se_train"].values,\n        result_table["CA_train"].values + result_table["CA-se_train"].values,\n        alpha=0.2,\n        color="darkorange",\n        lw=lw,\n    )\n    plt.plot(\n        result_table["Models"].values, result_table["CA"].values, label="Cross-validation score", color="navy"\n    )\n    plt.fill_between(\n        result_table["Models"].values,\n        result_table["CA"].values - result_table["CA-se"].values,\n        result_table["CA"].values + result_table["CA-se"].values,\n        alpha=0.2,\n        color="navy",\n        lw=lw,\n    )\n    plt.legend(loc="best")\n    plt.show()\n    \nif problem == "reg":\n    if score == "MSE":\n        plt.ylabel("MSE")\n        plt.plot(\n            result_table["Models"].values, result_table["MSE_train"].values, label="Training score", color="darkorange", lw=lw\n        )\n        plt.fill_between(\n            result_table["Models"].values,\n            result_table["MSE_train"].values - result_table["MSE-se_train"].values,\n            result_table["MSE_train"].values + result_table["MSE-se_train"].values,\n            alpha=0.2,\n            color="darkorange",\n            lw=lw,\n        )\n        plt.plot(\n            result_table["Models"].values, result_table["MSE"].values, label="Cross-validation score", color="navy"\n        )\n        plt.fill_between(\n            result_table["Models"].values,\n            result_table["MSE"].values - result_table["MSE-se"].values,\n            result_table["MSE"].values + result_table["MSE-se"].values,\n            alpha=0.2,\n            color="navy",\n            lw=lw,\n        )\n        plt.legend(loc="best")\n        plt.show()\n    else:\n        plt.ylabel("R2")\n        plt.plot(\n            result_table["Models"].values, result_table["R2_train"].values, label="Training score", color="darkorange", lw=lw\n        )\n        plt.fill_between(\n            result_table["Models"].values,\n            result_table["R2_train"].values - result_table["R2-se_train"].values,\n            result_table["R2_train"].values + result_table["R2-se_train"].values,\n            alpha=0.2,\n            color="darkorange",\n            lw=lw,\n        )\n        plt.plot(\n            result_table["Models"].values, result_table["R2"].values, label="Cross-validation score", color="navy"\n        )\n        plt.fill_between(\n            result_table["Models"].values,\n            result_table["R2"].values - result_table["R2-se"].values,\n            result_table["R2"].values + result_table["R2-se"].values,\n            alpha=0.2,\n            color="navy",\n            lw=lw,\n        )\n        plt.legend(loc="best")\n        plt.show()\n\n\n', 'splitterState': b'\x00\x00\x00\xff\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x02\xe0\x00\x00\x01i\x01\xff\xff\xff\xff\x01\x00\x00\x00\x02\x00', 'vimModeEnabled': False, '__version__': 2}</properties>
		<properties node_id="1" format="literal">{'controlAreaVisible': True, 'currentScriptIndex': 0, 'savedWidgetGeometry': b'\x01\xd9\xd0\xcb\x00\x03\x00\x00\x00\x00\x00\x05\x00\x00\x00\t\x00\x00\x04\xaa\x00\x00\x02\xe6\x00\x00\x00\x06\x00\x00\x00(\x00\x00\x04\xa9\x00\x00\x02\xe5\x00\x00\x00\x00\x00\x00\x00\x00\x07\x80\x00\x00\x00\x06\x00\x00\x00(\x00\x00\x04\xa9\x00\x00\x02\xe5', 'scriptLibrary': [{'name': '2D-Diagram_target_predictions2D.py', 'script': '# Shows 2D-diagram with training data and predictions out of connected model\n######################################################\n# Settings:\nprediction_model = 2         # Standard 1 for 1 connected model or the first connected model; \n#                              otherwise number of model, whose predictions should be shown\n#####################################################\n# File: Diagram_target_predictions2D.py\n\n"""\n* Widget input: \n- data with one feature, one target and (mandatory) one prediction (or several predictions) in Metas, only numeric values\n  e.g. from test and score widget or from predictions widget)\n- trained model (or several trained models) on classifier input\n\n* Widget output: -\n\nUsage:\n* to visually show prediction performance and over-/underfitting\n\n"""\n\nimport Orange\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nX = in_data.X\ny = in_data.Y\nmodels = in_classifiers\n#print(models)\nmeta_columns = in_data.metas.shape[1]\n\nx_min = X[:,0].min()\nx_max = X[:,0].max()\nX_model = np.linspace(x_min, x_max, 500)\n\nmodel_data = Orange.data.Table.from_numpy(in_data.domain, X_model.reshape(-1,1), Y = np.zeros(len(X_model)), metas = np.zeros(meta_columns*len(X_model)).reshape(-1,meta_columns))\n#model_data = Orange.data.Table.from_numpy(in_data.domain, X_model.reshape(-1,1), Y = np.zeros(len(X_model)), metas = np.zeros(len(X_model)).reshape(-1,1))\n\n\ny_hat = []\nfor model in models:\n    y_hat.append(model(model_data))\n    print(model.name)\n\ndef show(X, y, y_hat, name):\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.scatter(X[:,0], y, marker="o", c="red", label = "Training data")\n    ax.plot(X_model, y_hat,  c="blue", label = "Model predictions")\n    plt.title("Model: " + name)\n    plt.xlabel(in_data.domain[0])\n    plt.ylabel(in_data.domain[1])\n    plt.legend()\n    plt.show()\n\nshow(X=X, y=y, y_hat=y_hat[prediction_model-1], name=models[prediction_model-1].name)\n\n', 'filename': 'C:/Users/00613/Sync-Austausch/_ML/_Orange/Orange-Python-Scripts/Diagram_target_predictions2D.py'}], 'scriptText': '# Shows 2D-diagram with training data and predictions out of connected model\n######################################################\n# Settings:\nprediction_model = 2         # Standard 1 for 1 connected model or the first connected model; \n#                              otherwise number of model, whose predictions should be shown\n#####################################################\n# File: Diagram_target_predictions2D.py\n\n"""\n* Widget input: \n- data with one feature, one target and (mandatory) one prediction (or several predictions) in Metas, only numeric values\n  e.g. from test and score widget or from predictions widget)\n- trained model (or several trained models) on classifier input\n\n* Widget output: -\n\nUsage:\n* to visually show prediction performance and over-/underfitting\n\n"""\n\nimport Orange\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nX = in_data.X\ny = in_data.Y\nmodels = in_classifiers\n#print(models)\nmeta_columns = in_data.metas.shape[1]\n\nx_min = X[:,0].min()\nx_max = X[:,0].max()\nX_model = np.linspace(x_min, x_max, 500)\n\nmodel_data = Orange.data.Table.from_numpy(in_data.domain, X_model.reshape(-1,1), Y = np.zeros(len(X_model)), metas = np.zeros(meta_columns*len(X_model)).reshape(-1,meta_columns))\n#model_data = Orange.data.Table.from_numpy(in_data.domain, X_model.reshape(-1,1), Y = np.zeros(len(X_model)), metas = np.zeros(len(X_model)).reshape(-1,1))\n\n\ny_hat = []\nfor model in models:\n    y_hat.append(model(model_data))\n    print(model.name)\n\ndef show(X, y, y_hat, name):\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.scatter(X[:,0], y, marker="o", c="red", label = "Training data")\n    ax.plot(X_model, y_hat,  c="blue", label = "Model predictions")\n    plt.title("Model: " + name)\n    plt.xlabel(in_data.domain[0])\n    plt.ylabel(in_data.domain[1])\n    plt.legend()\n    plt.show()\n\nshow(X=X, y=y, y_hat=y_hat[prediction_model-1], name=models[prediction_model-1].name)\n\n', 'splitterState': b'\x00\x00\x00\xff\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x02\xe0\x00\x00\x01i\x01\xff\xff\xff\xff\x01\x00\x00\x00\x02\x00', 'vimModeEnabled': False, '__version__': 2}</properties>
		<properties node_id="2" format="literal">{'controlAreaVisible': True, 'currentScriptIndex': 0, 'savedWidgetGeometry': b'\x01\xd9\xd0\xcb\x00\x03\x00\x00\x00\x00\x02(\x00\x00\x00i\x00\x00\x05X\x00\x00\x03\x06\x00\x00\x02)\x00\x00\x00\x88\x00\x00\x05W\x00\x00\x03\x05\x00\x00\x00\x00\x00\x00\x00\x00\x07\x80\x00\x00\x02)\x00\x00\x00\x88\x00\x00\x05W\x00\x00\x03\x05', 'scriptLibrary': [{'name': 'Diagram_partition_boundaries', 'script': '# Shows 2D-diagram of data with partition boundaries\n# (no settings)\n#####################################################\n# File: Diagram_partition_boundaries.py\n\n"""\n* Widget input: \n- data with two numerical features, one categorical target\n- trained model on classifier input\n\n* Widget output: -\n\nUsage:\n* to visually show partition boundaries and over-/underfitting\n\n\nadapted from:\nPython Data Science Handbook – Essential Tools for Working with Data\nISBN 978-1491912058, 2017, from Jake VanderPlas\n"""\n\nimport Orange\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = in_data\nmodel = in_classifier\n\ndef visualize_classifier(model, data, ax=None, cmap=\'rainbow\'):\n    X = data.X\n    y = data.Y\n    ax = ax or plt.gca()\n    # show training data\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=100, cmap=cmap, clim=(y.min(), y.max()), zorder=3)\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n\n    # create grid and compute predictions on grid data\n    xx, yy = np.meshgrid(np.linspace(*xlim, num=200), np.linspace(*ylim, num=200))\n    Z = model((np.c_[xx.ravel(), yy.ravel()])).reshape(xx.shape)\n\n    # create diagram of grid data\n    n_classes = len(np.unique(y))\n    ax.scatter(xx,yy,Z)\n    contours = ax.contourf(xx, yy, Z, alpha=0.3, levels=np.arange(n_classes + 1) - 0.5, cmap=cmap,  zorder=1)\n    ax.set(xlim=xlim, ylim=ylim)\n    plt.xlabel(in_data.domain[0])\n    plt.ylabel(in_data.domain[1])\n    plt.show()\n    \nvisualize_classifier(model=model, data=data)\n', 'filename': None}], 'scriptText': '# Shows 2D-diagram of data with partition boundaries\n# (no settings)\n#####################################################\n# File: Diagram_partition_boundaries.py\n\n"""\n* Widget input: \n- data with two numerical features, one categorical target\n- trained model on classifier input\n\n* Widget output: -\n\nUsage:\n* to visually show partition boundaries and over-/underfitting\n\n\nadapted from:\nPython Data Science Handbook – Essential Tools for Working with Data\nISBN 978-1491912058, 2017, from Jake VanderPlas\n"""\n\nimport Orange\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = in_data\nmodel = in_classifier\n\ndef visualize_classifier(model, data, ax=None, cmap=\'rainbow\'):\n    X = data.X\n    y = data.Y\n    ax = ax or plt.gca()\n    # show training data\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=100, cmap=cmap, clim=(y.min(), y.max()), zorder=3)\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n\n    # create grid and compute predictions on grid data\n    xx, yy = np.meshgrid(np.linspace(*xlim, num=200), np.linspace(*ylim, num=200))\n    Z = model((np.c_[xx.ravel(), yy.ravel()])).reshape(xx.shape)\n\n    # create diagram of grid data\n    n_classes = len(np.unique(y))\n    ax.scatter(xx,yy,Z)\n    contours = ax.contourf(xx, yy, Z, alpha=0.3, levels=np.arange(n_classes + 1) - 0.5, cmap=cmap,  zorder=1)\n    ax.set(xlim=xlim, ylim=ylim)\n    plt.xlabel(in_data.domain[0])\n    plt.ylabel(in_data.domain[1])\n    plt.show()\n    \nvisualize_classifier(model=model, data=data)\n', 'splitterState': b'\x00\x00\x00\xff\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x02\xe0\x00\x00\x01i\x01\xff\xff\xff\xff\x01\x00\x00\x00\x02\x00', 'vimModeEnabled': False, '__version__': 2}</properties>
		<properties node_id="3" format="literal">{'controlAreaVisible': True, 'currentScriptIndex': 0, 'savedWidgetGeometry': b'\x01\xd9\xd0\xcb\x00\x03\x00\x00\x00\x00\x02\xbb\x00\x00\x00\xcd\x00\x00\x05\xcd\x00\x00\x03 \x00\x00\x02\xbc\x00\x00\x00\xec\x00\x00\x05\xcc\x00\x00\x03\x1f\x00\x00\x00\x00\x00\x00\x00\x00\x07\x80\x00\x00\x02\xbc\x00\x00\x00\xec\x00\x00\x05\xcc\x00\x00\x03\x1f', 'scriptLibrary': [{'name': 'Plot_Scatter_Matrix.py', 'script': '# Creates a scatter matrix plot of features\n###########################################\n# no settings\n#####################################################\n# File: Plot_Scatter_Matrix.py\n"""\n* Widget input: data with (not too many) numerical features\n* Widget output: -\n\nUsage:\n* Show a scatter matrix plot to see distribution and correlation of features\n\n"""\n\nfrom Orange.data.pandas_compat import table_from_frame,table_to_frame\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf= table_to_frame(in_data)\npd.plotting.scatter_matrix(df, alpha=0.5, diagonal = \'hist\')\nplt.suptitle(\'Scatter Matrix\')\nplt.show()\n\n# possible alternativ in pd.plotting.scatter_matrix\n# diagonal = \'kde\'    : core density estimation in diagonal\n# diagonal = \'hist\'   : histogram in diaagonal', 'filename': 'C:/Users/00613/Sync-Austausch/_ML/_Orange/Orange-Python-Scripts/Plot_Scatter_Matrix.py'}], 'scriptText': '# Creates a scatter matrix plot of features\n###########################################\n# no settings\n#####################################################\n# File: Plot_Scatter_Matrix.py\n"""\n* Widget input: data with (not too many) numerical features\n* Widget output: -\n\nUsage:\n* Show a scatter matrix plot to see distribution and correlation of features\n\n"""\n\nfrom Orange.data.pandas_compat import table_from_frame,table_to_frame\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf= table_to_frame(in_data)\npd.plotting.scatter_matrix(df, alpha=0.5, diagonal = \'hist\')\nplt.suptitle(\'Scatter Matrix\')\nplt.show()\n\n# possible alternativ in pd.plotting.scatter_matrix\n# diagonal = \'kde\'    : core density estimation in diagonal\n# diagonal = \'hist\'   : histogram in diaagonal', 'splitterState': b'\x00\x00\x00\xff\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x02\xe0\x00\x00\x01i\x01\xff\xff\xff\xff\x01\x00\x00\x00\x02\x00', 'vimModeEnabled': False, '__version__': 2}</properties>
		<properties node_id="4" format="literal">{'controlAreaVisible': True, 'currentScriptIndex': 0, 'savedWidgetGeometry': b'\x01\xd9\xd0\xcb\x00\x03\x00\x00\x00\x00\x02(\x00\x00\x00i\x00\x00\x05\xb1\x00\x00\x03=\x00\x00\x02)\x00\x00\x00\x88\x00\x00\x05\xb0\x00\x00\x03&lt;\x00\x00\x00\x00\x00\x00\x00\x00\x07\x80\x00\x00\x02)\x00\x00\x00\x88\x00\x00\x05\xb0\x00\x00\x03&lt;', 'scriptLibrary': [{'name': 'PolyFeatures.py', 'script': '# Generate polynomial and interaction features from input data\n########################################################\n# Settings:\ndegree = 2     # maximal degree of the polynomial features\n#################################################################\n# File: PolyFeatures.py\n\n"""\n* Widget input: data (only numerical features are used; categorical features are filtered out)\n* Widget output: data with additinal features, generated as polynomial combinations of the features\n\nUsage:\n* Create additional features to use linear regression for modeling nonlinear functions\n* Use to show the effect of model complexity on overfitting the data\n\n"""\n\n\nimport Orange\nfrom Orange.data import Domain, Table\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# use only numerical features, leave out other features:\ndomain = Domain([attr for attr in in_data.domain.attributes\n                 if attr.is_continuous], in_data.domain.class_vars)\nfiltered_data = Table.from_table(domain, in_data)\nX=filtered_data.X\n\n# generate new features with sklearn.preprocessing.PolynomialFeatures\npoly_features = PolynomialFeatures(degree=degree, include_bias=False)\nX_engi = poly_features.fit_transform(X)\n\n# create new domain and column-headers out of the feature_names, which are created by PolynomialFeatures\nattr_list = []\nfor i in range(len(poly_features.get_feature_names())):\n  attr1 = Orange.data.ContinuousVariable(poly_features.get_feature_names()[i])\n  attr_list.append(attr1)\n#  print(i)\n#  print(attr1.name)\nengi_domain = Domain(attr_list, in_data.domain.class_vars)\n\nout_data = Orange.data.Table(engi_domain, X_engi, in_data.Y)\n', 'filename': 'C:/Users/00613/Sync-Austausch/_ML/_Orange/Orange-Python-Scripts/PolyFeatures.py'}], 'scriptText': '# Generate polynomial and interaction features from input data\n########################################################\n# Settings:\ndegree = 2     # maximal degree of the polynomial features\n#################################################################\n# File: PolyFeatures.py\n\n"""\n* Widget input: data (only numerical features are used; categorical features are filtered out)\n* Widget output: data with additinal features, generated as polynomial combinations of the features\n\nUsage:\n* Create additional features to use linear regression for modeling nonlinear functions\n* Use to show the effect of model complexity on overfitting the data\n\n"""\n\n\nimport Orange\nfrom Orange.data import Domain, Table\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# use only numerical features, leave out other features:\ndomain = Domain([attr for attr in in_data.domain.attributes\n                 if attr.is_continuous], in_data.domain.class_vars)\nfiltered_data = Table.from_table(domain, in_data)\nX=filtered_data.X\n\n# generate new features with sklearn.preprocessing.PolynomialFeatures\npoly_features = PolynomialFeatures(degree=degree, include_bias=False)\nX_engi = poly_features.fit_transform(X)\n\n# create new domain and column-headers out of the feature_names, which are created by PolynomialFeatures\nattr_list = []\nfor i in range(len(poly_features.get_feature_names())):\n  attr1 = Orange.data.ContinuousVariable(poly_features.get_feature_names()[i])\n  attr_list.append(attr1)\n#  print(i)\n#  print(attr1.name)\nengi_domain = Domain(attr_list, in_data.domain.class_vars)\n\nout_data = Orange.data.Table(engi_domain, X_engi, in_data.Y)\n', 'splitterState': b'\x00\x00\x00\xff\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x02\xe0\x00\x00\x01i\x01\xff\xff\xff\xff\x01\x00\x00\x00\x02\x00', 'vimModeEnabled': False, '__version__': 2}</properties>
		<properties node_id="5" format="literal">{'controlAreaVisible': True, 'currentScriptIndex': 1, 'savedWidgetGeometry': b'\x01\xd9\xd0\xcb\x00\x03\x00\x00\x00\x00\x02(\x00\x00\x00i\x00\x00\x05\xf3\x00\x00\x03\x0c\x00\x00\x02)\x00\x00\x00\x88\x00\x00\x05\xf2\x00\x00\x03\x0b\x00\x00\x00\x00\x00\x00\x00\x00\x07\x80\x00\x00\x02)\x00\x00\x00\x88\x00\x00\x05\xf2\x00\x00\x03\x0b', 'scriptLibrary': [{'name': 'Display_MNIST-Image.py', 'script': '# Shows one instance of MNIST-data as image\n###########################################\n# no settings\n################################\n# File: Display_MNIST-Image.py\n\n"""\n* Widget input: data with (only) one instance of MNIST-data\n* Widget output: -\n\nUsage:\nTo visualize example data from MNIST (handwritten numbers)\n* X-data should be in 784 features representing the pixels in the 28x28 matrix.\n* Use Data Table Widget so select one row (and for example Confusion Matrix Widget in front of Data Table to select a group of instances).\n\n"""\n\n\nimport Orange\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nsome_digit = in_data.X\nsome_digit_image = some_digit.reshape(28, 28)\n\nplt.imshow(some_digit_image, cmap="binary")\nplt.axis("off")\nplt.show()\n', 'filename': 'C:/Users/00613/Sync-Austausch/_ML/_Orange/Orange-Python-Scripts/Display_MNIST-Image.py'}], 'scriptText': '# Shows one instance of MNIST-data as image\n###########################################\n# no settings\n################################\n# File: Display_MNIST-Image.py\n\n"""\n* Widget input: data with (only) one instance of MNIST-data\n* Widget output: -\n\nUsage:\nTo visualize example data from MNIST (handwritten numbers)\n* X-data should be in 784 features representing the pixels in the 28x28 matrix.\n* Use Data Table Widget to select one row (and for example Confusion Matrix Widget in front of Data Table to select a group of instances).\n\n"""\n\n\nimport Orange\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nsome_digit = in_data.X\nsome_digit_image = some_digit.reshape(28, 28)\n\nplt.imshow(some_digit_image, cmap="binary")\nplt.axis("off")\nplt.show()\n', 'splitterState': b'\x00\x00\x00\xff\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x02\xe0\x00\x00\x01i\x01\xff\xff\xff\xff\x01\x00\x00\x00\x02\x00', 'vimModeEnabled': False, '__version__': 2}</properties>
		<properties node_id="6" format="literal">{'controlAreaVisible': True, 'currentScriptIndex': 0, 'savedWidgetGeometry': b'\x01\xd9\xd0\xcb\x00\x03\x00\x00\x00\x00\x00w\x00\x00\x00\x0f\x00\x00\x05]\x00\x00\x036\x00\x00\x00x\x00\x00\x00.\x00\x00\x05\\\x00\x00\x035\x00\x00\x00\x00\x00\x00\x00\x00\x07\x80\x00\x00\x00x\x00\x00\x00.\x00\x00\x05\\\x00\x00\x035', 'scriptLibrary': [{'name': 'Poly_Regression.py', 'script': '# Polynomial Regression\n########################################################\n# Settings:\nname="poly. Reg., n= "         # Name of the learner/model in other widgets, e.g. Test and Score\ndegree = 1            # maximal degree of the polynomial features\nfit_intercept = True  # \ntype = "0"            # 0: ordinary Least square \n                      # 1: Lasso (L1-Regularization)\n                      # 2: Ridge (L2-Regularization)\nalpha = 0.0001        # parameter alpha for L1- and L2-Regularization only\n#################################################################\n# File: Poly_Regression.py\n\n"""\n* Widget input: \n    - (optional) data \n    - (optional) preprocessors on object-input\n\n* Widget output: \n    - learner on learner-output, if no data on input\n    - learner on learner-output and trained model on classifier-output, if data on input\n\n* Creates a learner for polynomial regression, which can be used e.g. in Test and Score-Widget\n* Additionally creates a trained model, if training data is connectet to data-input, e.g. for Predictions-Widget\n\n"""\n\nimport Orange\ndata = in_data\npreprocessors=in_objects\n\nif type == "0":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LinearRegressionLearner(fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorithm: ordinary Least square")\n\nelif type == "1":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LassoRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept, max_iter=2000),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorithm: Lasso (L1)")\n\nelif type == "2":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.RidgeRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorithm: Ridge (L2)")\nelse:\n    print("no algorithm specified")\n\nlearner.name = name\n\nif data != None:\n    classifier = learner(data)  \n    classifier.name = name\n\nout_learner = learner\nif data != None:\n    out_classifier = classifier \nelse:\n    out_classifier = None\nprint("\\nPreprocessors:")\nprint(preprocessors)\n\n\n# all possible settings for underlying learners for linear regression:\n#Orange.regression.linear.LinearRegressionLearner(preprocessors=None, fit_intercept=True)\n\n#Orange.regression.linear.RidgeRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=\'auto\', preprocessors=None)\n\n#Orange.regression.linear.LassoRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, preprocessors=None)\n', 'filename': 'C:/Users/00613/Sync-Austausch/_ML/_Orange/Orange-Python-Scripts/PolyFeatures.py'}], 'scriptText': '# Polynomial Regression\n########################################################\n# Settings:\nname="poly. Reg., n= "         # Name of the learner/model in other widgets, e.g. Test and Score\ndegree = 1            # maximal degree of the polynomial features\nfit_intercept = True  # \ntype = "0"            # 0: ordinary Least square \n                      # 1: Lasso (L1-Regularization)\n                      # 2: Ridge (L2-Regularization)\nalpha = 0.0001        # parameter alpha for L1- and L2-Regularization only\n#################################################################\n# File: Poly_Regression.py\n\n"""\n* Widget input: \n    - (optional) data \n    - (optional) preprocessors on object-input\n\n* Widget output: \n    - learner on learner-output, if no data on input\n    - learner on learner-output and trained model on classifier-output, if data on input\n\n* Creates a learner for polynomial regression, which can be used e.g. in Test and Score-Widget\n* Additionally creates a trained model, if training data is connectet to data-input, e.g. for Predictions-Widget\n\n"""\n\nimport Orange\ndata = in_data\npreprocessors=in_objects\n\nif type == "0":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LinearRegressionLearner(fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorithm: ordinary Least square")\n\nelif type == "1":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.LassoRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept, max_iter=2000),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorithm: Lasso (L1)")\n\nelif type == "2":\n    learner = Orange.regression.linear.PolynomialLearner(\n        learner=Orange.regression.linear.RidgeRegressionLearner(alpha=alpha,  \n            fit_intercept=fit_intercept),\n        degree=degree, preprocessors=preprocessors, include_bias=False)\n    print("\\nAlgorithm: Ridge (L2)")\nelse:\n    print("no algorithm specified")\n\nlearner.name = name\n\nif data != None:\n    classifier = learner(data)  \n    classifier.name = name\n\nout_learner = learner\nif data != None:\n    out_classifier = classifier \nelse:\n    out_classifier = None\nprint("\\nPreprocessors:")\nprint(preprocessors)\n\n\n# all possible settings for underlying learners for linear regression:\n#Orange.regression.linear.LinearRegressionLearner(preprocessors=None, fit_intercept=True)\n\n#Orange.regression.linear.RidgeRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=\'auto\', preprocessors=None)\n\n#Orange.regression.linear.LassoRegressionLearner(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, preprocessors=None)\n', 'splitterState': b'\x00\x00\x00\xff\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x024\x00\x00\x00\xac\x01\xff\xff\xff\xff\x01\x00\x00\x00\x02\x00', 'vimModeEnabled': False, '__version__': 2}</properties>
		<properties node_id="7" format="literal">{'controlAreaVisible': True, 'currentScriptIndex': 0, 'savedWidgetGeometry': b'\x01\xd9\xd0\xcb\x00\x03\x00\x00\x00\x00\x00\x10\x00\x00\x00\x06\x00\x00\x03\x88\x00\x00\x03\xe6\x00\x00\x00\x11\x00\x00\x00%\x00\x00\x03\x87\x00\x00\x03\xe5\x00\x00\x00\x00\x00\x00\x00\x00\x07\x80\x00\x00\x00\x11\x00\x00\x00%\x00\x00\x03\x87\x00\x00\x03\xe5', 'scriptLibrary': [{'name': 'Evaluation_Results_binary.py\n', 'script': '# Evaluation Results \n# (only for binary classification; \n# useful with manually adjusted threshold)\n########################################################\n# no Settings\n########################################################\n# File: Evaluation_Results_binary.py\n\n"""\n* Widget input: data\n    - predicted values as metas (and only this column in metas!)\n    - target\n    (use Select Columns Widget to shape the data accordingly)\n\n    Classes in target and predicted must be 1 for positiv and 0 for negative class!\n    Data may or may not contain features.\n\n* Widget output: \n    - no output, results are printed in this Widget.\n\n* Computes Confusion matrix and some more Evaluation results\n  out of data with predicted values and target.\n* Mainly, if predicted values are generated with Feature\n  Constructor Widget with a special Threshold.\n* Only for binary classification.\n\n"""\n\nimport Orange\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nfrom math import sqrt\n\ny_true = in_data.Y\ny_pred = in_data.metas\n\nmatrix = confusion_matrix(y_true, y_pred)\nCA = np.sum(np.diagonal(matrix))/np.sum(matrix)\nprecision = matrix[0,0]/np.sum(matrix[:,0])\nrecall = matrix[0,0]/np.sum(matrix[0,:])\nspecificity = matrix[1,1]/np.sum(matrix[1,:])\nF1_score = 2/((1/precision)+(1/recall))\n\n# calculate Matthews correlation coefficient\ntp = matrix[0,0]\ntn = matrix[1,1]\nfp = matrix[1,0]\nfn = matrix[0,1]\nMCC = (tp*tn-fp*fn)/sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n# There is also a sklearn-function for calculation of MCC out of y_true and y_pred.\n# But because confusion matrix is already calculated, this should be faster, I think.\n\nprint("")\nprint("-------------------------------")\nprint("Confusion matrix:")\nprint("-------------------------------")\nprint("                    predicted")\nprint("                   1          0")\nprint(f"actual  1   {matrix[0,0]:8}   {matrix[0,1]:8}")\nprint(f"        0   {matrix[1,0]:8}   {matrix[1,1]:8}")\n\nprint()\nprint(f"CA:            {CA:1.3}")\nprint(f"Precision:     {precision:1.3}")\nprint(f"Recall:        {recall:1.3}")\nprint(f"Specificity:   {specificity:1.3}")\nprint(f"F1_score:      {F1_score:1.3}")\nprint(f"MCC:           {MCC:1.3}")\n', 'filename': 'C:/Users/00613/Sync-Austausch/_ML/_Orange/Orange-Python-Scripts/Diagram_target_predictions2D.py'}], 'scriptText': '# Evaluation Results \n# (only for binary classification; \n# useful with manually adjusted threshold)\n########################################################\n# no Settings\n########################################################\n# File: Evaluation_Results_binary.py\n\n"""\n* Widget input: data\n    - predicted values as metas (and only this column in metas!)\n    - target\n    (use Select Columns Widget to shape the data accordingly)\n\n    Classes in target and predicted must be 1 for positiv and 0 for negative class!\n    Data may or may not contain features.\n\n* Widget output: \n    - no output, results are printed in this Widget.\n\n* Computes Confusion matrix and some more Evaluation results\n  out of data with predicted values and target.\n* Mainly, if predicted values are generated with Feature\n  Constructor Widget with a special Threshold.\n* Only for binary classification.\n\n"""\n\nimport Orange\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nfrom math import sqrt\n\ny_true = in_data.Y\ny_pred = in_data.metas\n\nmatrix = confusion_matrix(y_true, y_pred)\nCA = np.sum(np.diagonal(matrix))/np.sum(matrix)\nprecision = matrix[0,0]/np.sum(matrix[:,0])\nrecall = matrix[0,0]/np.sum(matrix[0,:])\nspecificity = matrix[1,1]/np.sum(matrix[1,:])\nF1_score = 2/((1/precision)+(1/recall))\n\n# calculate Matthews correlation coefficient\ntp = matrix[0,0]\ntn = matrix[1,1]\nfp = matrix[1,0]\nfn = matrix[0,1]\nMCC = (tp*tn-fp*fn)/sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n# There is also a sklearn-function for calculation of MCC out of y_true and y_pred.\n# But because confusion matrix is already calculated, this should be faster, I think.\n\nprint("")\nprint("-------------------------------")\nprint("Confusion matrix:")\nprint("-------------------------------")\nprint("                    predicted")\nprint("                   1          0")\nprint(f"actual  1   {matrix[0,0]:8}   {matrix[0,1]:8}")\nprint(f"        0   {matrix[1,0]:8}   {matrix[1,1]:8}")\n\nprint()\nprint(f"CA:            {CA:1.3}")\nprint(f"Precision:     {precision:1.3}")\nprint(f"Recall:        {recall:1.3}")\nprint(f"Specificity:   {specificity:1.3}")\nprint(f"F1_score:      {F1_score:1.3}")\nprint(f"MCC:           {MCC:1.3}")\n', 'splitterState': b'\x00\x00\x00\xff\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x02\x8d\x00\x00\x01\x0c\x01\xff\xff\xff\xff\x01\x00\x00\x00\x02\x00', 'vimModeEnabled': False, '__version__': 2}</properties>
		<properties node_id="8" format="literal">{'controlAreaVisible': True, 'currentScriptIndex': 0, 'savedWidgetGeometry': b'\x01\xd9\xd0\xcb\x00\x03\x00\x00\x00\x00\x00\x04\x00\x00\x00\xe8\x00\x00\x04&gt;\x00\x00\x03\x7f\x00\x00\x00\x05\x00\x00\x01\x07\x00\x00\x04=\x00\x00\x03~\x00\x00\x00\x00\x00\x00\x00\x00\x07\x80\x00\x00\x00\x05\x00\x00\x01\x07\x00\x00\x04=\x00\x00\x03~', 'scriptLibrary': [{'name': 'Learning_curve.py', 'script': '# Plot Learning Curve\n#############################################################\n# Settings:\nproblem = "reg"             # "class": classification or "reg": Regression\nscore = "R2"              # "R2" or "RMSE"; only for regression\nm = 20                      # number different training sizes\nsave_results = False        # True: save Excelfile with results; False: don\'t save\nfile_path = "E:/Downloads/" # file path for save_results\n#                             e.g. "E:/Downloads/" - with slash (!) also in Windows (and trailing slash)\n#############################################################\n# File: Learning_curve.py\n\n"""\n* Widget input: data and one learner\n* Widget output: -\n\n* Computes and shows learning curve for connected (1) learner.\n* Scores are CA (classification accuracy) for classification and R_squared (R2) or RMSE for regression.\n* Uses train-test-split, no cross validation.\n* Can throw an error, if connected learner is not compatible with type of class variable (numeric or categorical). Just connect a suitable learner.\n\n"""\n\nimport numpy as np\nimport Orange\nfrom Orange.data import Table\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\ndata = in_data.copy()\nlearner = in_learner\ndata.shuffle()\nX_train, X_test, y_train, y_test = train_test_split(data.X, data.Y, test_size=0.2)\ntrainset = Table.from_numpy(data.domain, X=X_train, Y=y_train)\ntestset = Table.from_numpy(data.domain, X=X_test, Y=y_test)\nn = len(trainset)\nspacing = np.linspace(5, n, m).round()   # spacing is np.array of floats\ntrain_sizes = [int(i) for i in spacing]  # list of integers as needed for slicing of y_train\n\n# compute and show learning curve\n#----------------------------------------------\ndef plot_learning_curves(learner, trainset, testset):\n    \n    train_errors, val_errors = [], []\n    for m in train_sizes:\n        model = learner(trainset[:m])\n        y_pred_test = model(testset)\n        y_pred_train = model(trainset[:m])\n        if problem == "reg":\n            if score == "RMSE":\n                train_errors.append(metrics.mean_squared_error(y_train[:m], y_pred_train, squared=False))\n                val_errors.append(metrics.mean_squared_error(y_test, y_pred_test, squared=False))\n            if score == "R2":\n                train_errors.append(metrics.r2_score(y_train[:m], y_pred_train))\n                val_errors.append(metrics.r2_score(y_test, y_pred_test))\n        if problem == "class": # CA\n            train_errors.append(metrics.accuracy_score(y_train[:m], y_pred_train))\n            val_errors.append(metrics.accuracy_score(y_test, y_pred_test))\n    \n    plt.plot(train_sizes, train_errors, "-o", linewidth=2, label="Training score", color="darkorange")\n    plt.plot(train_sizes, val_errors, "-", linewidth=3, label="Test score", color="navy")\n    plt.title("Learning Curve")\n    if problem == "reg" and score == "RMSE": plt.ylabel("RMSE")\n    if problem == "reg" and score == "R2": plt.ylabel("R2")\n    if problem == "class": plt.ylabel("CA")\n    plt.xlabel("Number of training samples")\n    plt.legend(loc="best")\n    plt.show()\n    return train_errors, val_errors\n\ntrain_errors, val_errors = plot_learning_curves(learner, trainset, testset)\n\n# aggregate results\n#----------------------------------------------\nresult_table = pd.DataFrame([])\nresult_table["Train sizes"] = train_sizes\nif problem == "class":\n    result_table["CA_train"] = train_errors\n    result_table["CA_test"] = val_errors\nif problem == "reg" and score == "RMSE":\n    result_table["RMSE_train"] = train_errors\n    result_table["RMSE_test"] = val_errors\nif problem == "reg" and score == "R2":\n    result_table["R2_train"] = train_errors\n    result_table["R2_test"] = val_errors\n    \n# Print to table and save data\n#-----------------------------\nprint("---------------------------")\nprint("Results of Learning Curve")\nprint("---------------------------")\nprint("Number of instances: ", len(data))\nprint("")\nif problem == "class":\n    print(result_table)\nif problem == "reg":\n    print(result_table)\n\nif save_results:\n    file_name_path = file_path + "Results-" + datetime.now().strftime("%Y-%m-%d_%H-%M-%S") + ".xlsx"\n    print("Results in: ",file_name_path)\n    with pd.ExcelWriter(file_name_path) as writer:\n        result_table.to_excel(writer)\n', 'filename': 'C:/Users/00613/Sync-Austausch/_ML/_Orange/Orange-Python-Scripts/Learning_curve.py'}], 'scriptText': '# Plot Learning Curve\n#############################################################\n# Settings:\nproblem = "reg"             # "class": classification or "reg": Regression\nscore = "R2"              # "R2" or "RMSE"; only for regression\nm = 20                      # number different training sizes\nsave_results = False        # True: save Excelfile with results; False: don\'t save\nfile_path = "E:/Downloads/" # file path for save_results\n#                             e.g. "E:/Downloads/" - with slash (!) also in Windows (and trailing slash)\n#############################################################\n# File: Learning_curve.py\n\n"""\n* Widget input: data and one learner\n* Widget output: -\n\n* Computes and shows learning curve for connected (1) learner.\n* Scores are CA (classification accuracy) for classification and R_squared (R2) or RMSE for regression.\n* Uses train-test-split, no cross validation.\n* Can throw an error, if connected learner is not compatible with type of class variable (numeric or categorical). Just connect a suitable learner.\n\n"""\n\nimport numpy as np\nimport Orange\nfrom Orange.data import Table\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\ndata = in_data.copy()\nlearner = in_learner\ndata.shuffle()\nX_train, X_test, y_train, y_test = train_test_split(data.X, data.Y, test_size=0.2)\ntrainset = Table.from_numpy(data.domain, X=X_train, Y=y_train)\ntestset = Table.from_numpy(data.domain, X=X_test, Y=y_test)\nn = len(trainset)\nspacing = np.linspace(5, n, m).round()   # spacing is np.array of floats\ntrain_sizes = [int(i) for i in spacing]  # list of integers as needed for slicing of y_train\n\n# compute and show learning curve\n#----------------------------------------------\ndef plot_learning_curves(learner, trainset, testset):\n    \n    train_errors, val_errors = [], []\n    for m in train_sizes:\n        model = learner(trainset[:m])\n        y_pred_test = model(testset)\n        y_pred_train = model(trainset[:m])\n        if problem == "reg":\n            if score == "RMSE":\n                train_errors.append(metrics.mean_squared_error(y_train[:m], y_pred_train, squared=False))\n                val_errors.append(metrics.mean_squared_error(y_test, y_pred_test, squared=False))\n            if score == "R2":\n                train_errors.append(metrics.r2_score(y_train[:m], y_pred_train))\n                val_errors.append(metrics.r2_score(y_test, y_pred_test))\n        if problem == "class": # CA\n            train_errors.append(metrics.accuracy_score(y_train[:m], y_pred_train))\n            val_errors.append(metrics.accuracy_score(y_test, y_pred_test))\n    \n    plt.plot(train_sizes, train_errors, "-o", linewidth=2, label="Training score", color="darkorange")\n    plt.plot(train_sizes, val_errors, "-", linewidth=3, label="Test score", color="navy")\n    plt.title("Learning Curve")\n    if problem == "reg" and score == "RMSE": plt.ylabel("RMSE")\n    if problem == "reg" and score == "R2": plt.ylabel("R2")\n    if problem == "class": plt.ylabel("CA")\n    plt.xlabel("Number of training samples")\n    plt.legend(loc="best")\n    plt.show()\n    return train_errors, val_errors\n\ntrain_errors, val_errors = plot_learning_curves(learner, trainset, testset)\n\n# aggregate results\n#----------------------------------------------\nresult_table = pd.DataFrame([])\nresult_table["Train sizes"] = train_sizes\nif problem == "class":\n    result_table["CA_train"] = train_errors\n    result_table["CA_test"] = val_errors\nif problem == "reg" and score == "RMSE":\n    result_table["RMSE_train"] = train_errors\n    result_table["RMSE_test"] = val_errors\nif problem == "reg" and score == "R2":\n    result_table["R2_train"] = train_errors\n    result_table["R2_test"] = val_errors\n    \n# Print to table and save data\n#-----------------------------\nprint("---------------------------")\nprint("Results of Learning Curve")\nprint("---------------------------")\nprint("Number of instances: ", len(data))\nprint("")\nif problem == "class":\n    print(result_table)\nif problem == "reg":\n    print(result_table)\n\nif save_results:\n    file_name_path = file_path + "Results-" + datetime.now().strftime("%Y-%m-%d_%H-%M-%S") + ".xlsx"\n    print("Results in: ",file_name_path)\n    with pd.ExcelWriter(file_name_path) as writer:\n        result_table.to_excel(writer)\n', 'splitterState': b'\x00\x00\x00\xff\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x01\x1a\x00\x00\x016\x01\xff\xff\xff\xff\x01\x00\x00\x00\x02\x00', 'vimModeEnabled': False, '__version__': 2}</properties>
		<properties node_id="9" format="literal">{'controlAreaVisible': True, 'currentScriptIndex': 0, 'savedWidgetGeometry': b'\x01\xd9\xd0\xcb\x00\x03\x00\x00\x00\x00\x00\xb6\x00\x00\x02F\x00\x00\x05%\x00\x00\x04@\x00\x00\x00\xb6\x00\x00\x02F\x00\x00\x05%\x00\x00\x04@\x00\x00\x00\x00\x00\x00\x00\x00\x07\x80\x00\x00\x00\xb6\x00\x00\x02F\x00\x00\x05%\x00\x00\x04@', 'scriptLibrary': [{'name': 'Diagram_target_predictions.py', 'script': '# Shows 2D- or 3D-diagram of data\n#################################\n# Settings:\nmetadata = True             # False  if metadata/predictions should not be shown\n#                             True   if metadata/predictions should be shown\nenforce_2D = False          # True   if 2D-diagram should be shown, although \n#                             two dimensions (features) are available in input data\nscatter_prediction = True   # True -&gt; dots, False -&gt; lines for metadata/predictions \n#                             in 2D-diagram\nprediction_model = 1        # Standard 1 for 1 column of predictions in metadata or\n#                             first column of predictions; \n#                             otherwise number of column, whose predictions should be shown\n#####################################################\n# File: Diagram_target_predictions.py\n\n"""\nWidget input: data with one or two numerical features, numerial target and (optional) one or several predictions, stored as metadata\nWidget output: -\n\nUsage:\n* Show numerical data with target and one feature in a scatterplot together with predicted data\n* Show numerical data with target and two features in a 3D-scatterplot together with predicted data\n* Predictions can be shown as dots or as line in 2D- and as surface in 3D-plots.\n\n"""\n\n\nimport Orange\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nX = in_data.X\ny = in_data.Y\nif metadata: \n    y_hat = in_data.metas[:,prediction_model-1] \n    model_name = in_data.domain.metas[prediction_model-1].name\nelse: \n    y_hat=None\n    model_name = ""\n\nif X.shape[1] ==2: display_3D = True \nelse: display_3D = False\n\nif enforce_2D: display_3D = False\n\ndef show(X, y, y_hat, name=model_name):\n    if display_3D:\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection=\'3d\')\n        ax.scatter(X[:,0], X[:,1], y, s= 10, marker="o", c="red", label = "Training data")\n        if metadata: ax.plot_trisurf(X[:,0], X[:,1], y_hat, alpha=0.5)\n        ax.set_xlabel(in_data.domain[0])\n        ax.set_ylabel(in_data.domain[1])\n        ax.set_zlabel(in_data.domain.class_var.name)\n        ax.set_xlim(0, X[:,0].max()+2)\n        ax.set_ylim(0, X[:,1].max()+2)\n        #ax.set_zlim(0, 400)\n        plt.title(name)\n        plt.legend()\n        plt.show()\n    else:\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n        ax.scatter(X[:,0], y, marker="o", c="red", label = "Training data")\n        if metadata:\n            if scatter_prediction:\n                ax.scatter(X[:,0], y_hat, marker="+", c="blue", label = "Model predictions")\n            else:\n                ax.plot(X[:,0], y_hat,  c="blue", label = "Model predictions")\n        plt.xlabel(in_data.domain[0])\n        plt.ylabel(in_data.domain.class_var.name)\n        plt.title(name)\n        plt.legend()\n        plt.show()\n\nshow(X=X, y=y, y_hat=y_hat, name=model_name)\n', 'filename': 'C:/Users/00613/Sync-Austausch/_ML/_Orange/Orange-Python-Scripts/Diagram_target_predictions.py'}], 'scriptText': '# Shows 2D- or 3D-diagram of data\n#################################\n# Settings:\nmetadata = True             # False  if metadata/predictions should not be shown\n#                             True   if metadata/predictions should be shown\nenforce_2D = False          # True   if 2D-diagram should be shown, although \n#                             two dimensions (features) are available in input data\nscatter_prediction = True   # True -&gt; dots, False -&gt; lines for metadata/predictions \n#                             in 2D-diagram\nprediction_model = 1        # Standard 1 for 1 column of predictions in metadata or\n#                             first column of predictions; \n#                             otherwise number of column, whose predictions should be shown\n#####################################################\n# File: Diagram_target_predictions.py\n\n"""\nWidget input: data with one or two numerical features, numerial target and (optional) one or several predictions, stored as metadata\nWidget output: -\n\nUsage:\n* Show numerical data with target and one feature in a scatterplot together with predicted data\n* Show numerical data with target and two features in a 3D-scatterplot together with predicted data\n* Predictions can be shown as dots or as line in 2D- and as surface in 3D-plots.\n\n"""\n\n\nimport Orange\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nX = in_data.X\ny = in_data.Y\nif metadata: \n    y_hat = in_data.metas[:,prediction_model-1] \n    model_name = in_data.domain.metas[prediction_model-1].name\nelse: \n    y_hat=None\n    model_name = ""\n\nif X.shape[1] ==2: display_3D = True \nelse: display_3D = False\n\nif enforce_2D: display_3D = False\n\ndef show(X, y, y_hat, name=model_name):\n    if display_3D:\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection=\'3d\')\n        ax.scatter(X[:,0], X[:,1], y, s= 10, marker="o", c="red", label = "Training data")\n        if metadata: ax.plot_trisurf(X[:,0], X[:,1], y_hat, alpha=0.5)\n        ax.set_xlabel(in_data.domain[0])\n        ax.set_ylabel(in_data.domain[1])\n        ax.set_zlabel(in_data.domain.class_var.name)\n        ax.set_xlim(0, X[:,0].max()+2)\n        ax.set_ylim(0, X[:,1].max()+2)\n        #ax.set_zlim(0, 400)\n        plt.title(name)\n        plt.legend()\n        plt.show()\n    else:\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n        ax.scatter(X[:,0], y, marker="o", c="red", label = "Training data")\n        if metadata:\n            if scatter_prediction:\n                ax.scatter(X[:,0], y_hat, marker="+", c="blue", label = "Model predictions")\n            else:\n                ax.plot(X[:,0], y_hat,  c="blue", label = "Model predictions")\n        plt.xlabel(in_data.domain[0])\n        plt.ylabel(in_data.domain.class_var.name)\n        plt.title(name)\n        plt.legend()\n        plt.show()\n\nshow(X=X, y=y, y_hat=y_hat, name=model_name)\n', 'splitterState': b'\x00\x00\x00\xff\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x01S\x00\x00\x00\x80\x01\xff\xff\xff\xff\x01\x00\x00\x00\x02\x00', 'vimModeEnabled': False, '__version__': 2}</properties>
		<properties node_id="10" format="literal">{'controlAreaVisible': True, 'currentScriptIndex': 0, 'savedWidgetGeometry': b'\x01\xd9\xd0\xcb\x00\x03\x00\x00\x00\x00\x02\x9b\x00\x00\x01t\x00\x00\x06\x89\x00\x00\x03\xff\x00\x00\x02\x9b\x00\x00\x01t\x00\x00\x06\x89\x00\x00\x03\xff\x00\x00\x00\x00\x00\x00\x00\x00\x07\x80\x00\x00\x02\x9b\x00\x00\x01t\x00\x00\x06\x89\x00\x00\x03\xff', 'scriptLibrary': [{'name': 'Evaluation_results', 'script': '# Confusion Matrix and some more Evaluation Results\n####################################################\n# Settings:\nTarget_Class = "Scrap"\n# Target Class or positive Class for calculation \n# of Evaluation Results other than Confusion matrix\n# and CA\n\n#####################################################\n# File: Evaluation_results.py\n\n"""\n* Widget input: data\n    - predicted values as metas (must be the first item in metas!)\n    (If necessary, use Select Columns Widget to shape the data accordingly)\n    - target\n    Sequence of classes must be the same in target and metas! \n    Use Edit Domain widget to ensure this.\n\n    Data may or may not contain features.\n\nUsage:\n* Widget output: \n    - no output, results are printed in this Widget.\n\n* Select a Target Class or positive Class for calculation.\n* Computes Confusion matrix and some more Evaluation results\n  out of data with predicted values and target.\n* For example, if predicted values are generated with Feature\n  Constructor Widget with a special Threshold to tweak a model.\n\n"""\n\n\nimport Orange\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nfrom math import sqrt\n\ndata = in_data\n#X = data.X\n#y = data.Y\n\nif Target_Class in data.domain.class_var.values:\n\n    y_true = data.Y.astype(int)\n    y_pred = data.metas[:,0].astype(int)\n\n    # calculating confusion matrix and CA\n    cfmatrix = confusion_matrix(y_true, y_pred)\n    CA = np.sum(np.diagonal(cfmatrix))/np.sum(cfmatrix)\n\n    # calculating the other performance criteria\n    positive_class = data.domain.class_var.values.index(Target_Class)\n    matrix = np.zeros([2,2])\n    matrix[0,0] = cfmatrix[positive_class, positive_class]\n    matrix[1,0] = np.sum(cfmatrix[:, positive_class]) - matrix[0,0]\n    matrix[0,1] = np.sum(cfmatrix[positive_class, :]) - matrix[0,0]\n    matrix[1,1] = np.sum(cfmatrix) - matrix[0,0] - matrix[0,1] - matrix[1,0]\n\n    precision = matrix[0,0]/np.sum(matrix[:,0])\n    recall = matrix[0,0]/np.sum(matrix[0,:])\n    specificity = matrix[1,1]/np.sum(matrix[1,:])\n    F1_score = 2/((1/precision)+(1/recall))\n\n    # calculate Matthews correlation coefficient\n    tp = matrix[0,0]\n    tn = matrix[1,1]\n    fp = matrix[1,0]\n    fn = matrix[0,1]\n    MCC = (tp*tn-fp*fn)/sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n    # There is also a sklearn-function for calculation of MCC out of y_true and y_pred.\n    # But because confusion matrix is already calculated, this should be faster, I think.\n\n    print("")\n    print("-------------------------------")\n    print("Confusion matrix:")\n    print("-------------------------------")\n    print("            \\u2193 actual predicted \\u2192")\n    print("                     ", end="")\n\n    for item in data.domain.metas[0].values:\n        print(item.ljust(20)[:20]+" ", end="")\n\n    num_classes = len(data.domain.class_var.values)\n    for i in range(num_classes):\n        print("\\n"+data.domain.class_var.values[i].rjust(20)[:20]+" ", end="")\n        for j in range(num_classes):\n            print(f"{cfmatrix[i,j]}".ljust(21), end="")\n\n    print()\n    print()\n    print(f"CA:            {CA:1.3}")\n    print(f"Precision:     {precision:1.3}")\n    print(f"Recall:        {recall:1.3}")\n    print(f"Specificity:   {specificity:1.3}")\n    print(f"F1_score:      {F1_score:1.3}")\n    print(f"MCC:           {MCC:1.3}")\n\n    print("Target Class (positive Class): ", Target_Class)\n\nelse:\n    print("-------------------------------------------------")\n    print("Select a Target Class out of the current Classes:")\n    for item in data.domain.class_var.values:\n        print(item +", ", end="")\n    print("-------------------------------------------------")\n', 'filename': 'C:/Users/00613/Sync-Austausch/_ML/_Orange/Orange-Python-Scripts/Diagram_target_predictions.py'}], 'scriptText': '# Confusion Matrix and some more Evaluation Results\n####################################################\n# Settings:\nTarget_Class = "Scrap"\n# Target Class or positive Class for calculation \n# of Evaluation Results other than Confusion matrix\n# and CA\n\n#####################################################\n# File: Evaluation_results.py\n\n"""\n* Widget input: data\n    - predicted values as metas (must be the first item in metas!)\n    (If necessary, use Select Columns Widget to shape the data accordingly)\n    - target\n    Sequence of classes must be the same in target and metas! \n    Use Edit Domain widget to ensure this.\n\n    Data may or may not contain features.\n\nUsage:\n* Widget output: \n    - no output, results are printed in this Widget.\n\n* Select a Target Class or positive Class for calculation.\n* Computes Confusion matrix and some more Evaluation results\n  out of data with predicted values and target.\n* For example, if predicted values are generated with Feature\n  Constructor Widget with a special Threshold to tweak a model.\n\n"""\n\n\nimport Orange\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nfrom math import sqrt\n\ndata = in_data\n#X = data.X\n#y = data.Y\n\nif Target_Class in data.domain.class_var.values:\n\n    y_true = data.Y.astype(int)\n    y_pred = data.metas[:,0].astype(int)\n\n    # calculating confusion matrix and CA\n    cfmatrix = confusion_matrix(y_true, y_pred)\n    CA = np.sum(np.diagonal(cfmatrix))/np.sum(cfmatrix)\n\n    # calculating the other performance criteria\n    positive_class = data.domain.class_var.values.index(Target_Class)\n    matrix = np.zeros([2,2])\n    matrix[0,0] = cfmatrix[positive_class, positive_class]\n    matrix[1,0] = np.sum(cfmatrix[:, positive_class]) - matrix[0,0]\n    matrix[0,1] = np.sum(cfmatrix[positive_class, :]) - matrix[0,0]\n    matrix[1,1] = np.sum(cfmatrix) - matrix[0,0] - matrix[0,1] - matrix[1,0]\n\n    precision = matrix[0,0]/np.sum(matrix[:,0])\n    recall = matrix[0,0]/np.sum(matrix[0,:])\n    specificity = matrix[1,1]/np.sum(matrix[1,:])\n    F1_score = 2/((1/precision)+(1/recall))\n\n    # calculate Matthews correlation coefficient\n    tp = matrix[0,0]\n    tn = matrix[1,1]\n    fp = matrix[1,0]\n    fn = matrix[0,1]\n    MCC = (tp*tn-fp*fn)/sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n    # There is also a sklearn-function for calculation of MCC out of y_true and y_pred.\n    # But because confusion matrix is already calculated, this should be faster, I think.\n\n    print("")\n    print("-------------------------------")\n    print("Confusion matrix:")\n    print("-------------------------------")\n    print("            \\u2193 actual predicted \\u2192")\n    print("                     ", end="")\n\n    for item in data.domain.metas[0].values:\n        print(item.ljust(20)[:20]+" ", end="")\n\n    num_classes = len(data.domain.class_var.values)\n    for i in range(num_classes):\n        print("\\n"+data.domain.class_var.values[i].rjust(20)[:20]+" ", end="")\n        for j in range(num_classes):\n            print(f"{cfmatrix[i,j]}".ljust(21), end="")\n\n    print()\n    print()\n    print(f"CA:            {CA:1.3}")\n    print(f"Precision:     {precision:1.3}")\n    print(f"Recall:        {recall:1.3}")\n    print(f"Specificity:   {specificity:1.3}")\n    print(f"F1_score:      {F1_score:1.3}")\n    print(f"MCC:           {MCC:1.3}")\n\n    print("Target Class (positive Class): ", Target_Class)\n\nelse:\n    print("-------------------------------------------------")\n    print("Select a Target Class out of the current Classes:")\n    for item in data.domain.class_var.values:\n        print(item +", ", end="")\n    print("-------------------------------------------------")\n', 'splitterState': b'\x00\x00\x00\xff\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x00\xf2\x00\x00\x01r\x01\xff\xff\xff\xff\x01\x00\x00\x00\x02\x00', 'vimModeEnabled': False, '__version__': 2}</properties>
		<properties node_id="11" format="literal">{'controlAreaVisible': True, 'currentScriptIndex': 0, 'savedWidgetGeometry': b'\x01\xd9\xd0\xcb\x00\x03\x00\x00\x00\x00\x00\x0e\x00\x00\x008\x00\x00\x03\x9f\x00\x00\x03&gt;\x00\x00\x00\x0f\x00\x00\x00W\x00\x00\x03\x9e\x00\x00\x03=\x00\x00\x00\x00\x00\x00\x00\x00\x07\x80\x00\x00\x00\x0f\x00\x00\x00W\x00\x00\x03\x9e\x00\x00\x03=', 'scriptLibrary': [{'name': 'Validation_curve.py', 'script': '# Plot Validation Curve\n#######################\n# Settings:\nscore = "MSE"                # "R2" or "MSE"; only for regression\nsave_results = False        # True: save Excelfile with results; False: don\'t save\nfile_path = "E:/Downloads/" # file path for save_results\n#                             e.g. "E:/Downloads/" - with slash (!) also in Windows (and trailing slash)\n#####################################################\n# File: Validation_curve.py\n\n"""\n* Widget input: Predictions from Test &amp; Score with Cross validation\n  and optional: Predictions from Test &amp; Score with test on training data\n* Widget output: -\n\n* Computes and shows validation curve for learners out of cross validation by widget Test &amp; Score\n* Uses standard error to indicate variability of the results.\n* Scores are CA (classification accuracy) for classification and R_squared (R2) or MSE for regression.\n* Uses names of learners as x-labels.\n\nUsage: \n* mainly to compare different learner settings for manual hyper parameter optimization.\n* e.g. several learners of the same type, but with different values of a certain hyper parameter\n* Use name in learner widget to indicate different learners\n\n"""\n\nimport numpy as np\nfrom Orange.data import Table\nfrom Orange.evaluation import TestOnTestData\nimport Orange\nfrom sklearn import metrics\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\ndata = in_data.copy()\nn = len(data)\ny_true = data.Y\nn_meta_cols = len(data.domain.metas)\n\nif data.domain.class_var.is_continuous:\n    problem = "reg"\n    n_classes = 0\n    n_learner = n_meta_cols-1\nelse:\n    problem = "class"\n    n_classes = len(data.domain.class_var.values)\n    n_learner = int((n_meta_cols-1)/(n_classes+1))\n\nif in_object == None:\n    show_traindata = False\nelse:\n    show_traindata = True\n    traindata = in_object.copy()\n\nfolds = data.metas[:, n_learner*(n_classes+1)]  # column with folds-indicator\nk = len(np.unique(folds))  # number of folds\n\nca = np.zeros([n_learner,k])\nmse= np.zeros([n_learner,k])\nr2 = np.zeros([n_learner,k])\nr2_total = np.zeros([n_learner])\nca_train = np.zeros([n_learner,k])\nmse_train = np.zeros([n_learner,k])\nr2_train = np.zeros([n_learner,k])\n\n# Split data in folds and calculate metrics\n# -----------------------------------------\nif problem == "class":\n    for fold in range(k):\n        y_test = y_true[folds==fold].astype(int)\n        y_pred_test = data.metas[:,0:n_learner][folds==fold].astype(int)\n        for learner in range(n_learner):  # calculate metrics\n            # Metrics see https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n            # use only CA to avoid problems with multiclass labels\n            ca[learner, fold] = metrics.accuracy_score(y_test, y_pred_test[:, learner])\n\nelif problem == "reg":\n    for fold in range(k):\n        y_test = y_true[folds==fold]\n        y_pred_test = data.metas[:,0:n_learner][folds==fold]\n        for learner in range(n_learner):\n            mse[learner,fold] = metrics.mean_squared_error(y_test, y_pred_test[:, learner])\n            r2[learner,fold] = metrics.r2_score(y_test, y_pred_test[:, learner])\n            r2_total[learner] = metrics.r2_score(y_true, data.metas[:, learner])\n\nif show_traindata: \n    if problem == "class":\n        y_train = traindata.Y.astype(int)\n        y_pred_train = traindata.metas[:,0:n_learner].astype(int)\n        for learner in range(n_learner):\n            ca_train[learner] = metrics.accuracy_score(y_train, y_pred_train[:, learner])\n    elif problem == "reg":\n        y_train = traindata.Y\n        y_pred_train = traindata.metas[:,0:n_learner]\n        for learner in range(n_learner):\n            mse_train[learner] = metrics.mean_squared_error(y_train, y_pred_train[:, learner])\n            r2_train[learner] = metrics.r2_score(y_train, y_pred_train[:, learner])\n\n# Aggregate results (means and standard errors)\n#----------------------------------------------\nresult_table = pd.DataFrame([])\n#result_table["Models"] = [F"{learners[learner]}" for learner,_ in enumerate(learners)]\nresult_table["Models"] = [F"{data.domain.metas[learner]}" for learner in range(n_learner)]\n#print(result_table["Models"])\nif problem == "class":\n    result_table["CA"] = [ca[learner].mean() for learner in range(n_learner)]\n    result_table["CA-se"] = [ca[learner].std(ddof=1)/k for learner in range(n_learner)]\n    if show_traindata: result_table["CA_train"] = [ca_train[learner].mean() for learner in range(n_learner)]\n    \nif problem == "reg":\n    result_table["MSE"] = [mse[learner].mean() for learner in range(n_learner)]\n    result_table["MSE-se"] = [mse[learner].std(ddof=1)/k for learner in range(n_learner)]\n    result_table["R2"] = [r2[learner].mean() for learner in range(n_learner)]\n    result_table["R2"] = r2_total\n    result_table["R2-se"] = [r2[learner].std(ddof=1)/k for learner in range(n_learner)]\n    if show_traindata:\n        result_table["MSE_train"] = [mse_train[learner].mean() for learner in range(n_learner)]\n        result_table["R2_train"] = [r2_train[learner].mean() for learner in range(n_learner)]\n\n# Print to table and save data\n#-----------------------------\nprint("---------------------------")\nprint("Results of Cross validation")\nprint("---------------------------")\nprint("Number of instances: ", len(data))\nprint("Number of folds    : ", k)\nprint("")\nif problem == "class":\n    print(result_table)\nif problem == "reg":\n    print(result_table[["Models", "MSE", "MSE-se", "R2", "R2-se"]])\n    if show_traindata: print(result_table[["Models", "MSE_train", "R2_train"]])\n\nif save_results:\n    file_name_path = file_path + "Results-" + datetime.now().strftime("%Y-%m-%d_%H-%M-%S") + ".xlsx"\n    print("Results in: ",file_name_path)\n    with pd.ExcelWriter(file_name_path) as writer:\n        result_table.to_excel(writer)\n\n# Create Diagram\n#---------------\nfig = plt.figure(figsize=(8,6))\nplt.title("Validation Curve")\nlw = 2\nif problem == "class":\n    plt.ylabel("CA")\n    if show_traindata: plt.plot(\n        result_table["Models"].values, result_table["CA_train"].values, label="Training score", color="darkorange", lw=lw\n    )\n    plt.plot(\n        result_table["Models"].values, result_table["CA"].values, label="Cross-validation score", color="navy"\n    )\n    plt.fill_between(\n        result_table["Models"].values,\n        result_table["CA"].values - result_table["CA-se"].values,\n        result_table["CA"].values + result_table["CA-se"].values,\n        alpha=0.2,\n        color="navy",\n        lw=lw,\n    )\n    plt.legend(loc="best")\n    plt.show()\n    \nif problem == "reg":\n    if score == "MSE":\n        plt.ylabel("MSE")\n        if show_traindata: plt.plot(\n            result_table["Models"].values, result_table["MSE_train"].values, label="Training score", color="darkorange", lw=lw\n        )\n        plt.plot(\n            result_table["Models"].values, result_table["MSE"].values, label="Cross-validation score", color="navy"\n        )\n        plt.fill_between(\n            result_table["Models"].values,\n            result_table["MSE"].values - result_table["MSE-se"].values,\n            result_table["MSE"].values + result_table["MSE-se"].values,\n            alpha=0.2,\n            color="navy",\n            lw=lw,\n        )\n        plt.legend(loc="best")\n        plt.show()\n    else:\n        plt.ylabel("R2")\n        if show_traindata: plt.plot(\n            result_table["Models"].values, result_table["R2_train"].values, label="Training score", color="darkorange", lw=lw\n        )\n        plt.plot(\n            result_table["Models"].values, result_table["R2"].values, label="Cross-validation score", color="navy"\n        )\n        plt.fill_between(\n            result_table["Models"].values,\n            result_table["R2"].values - result_table["R2-se"].values,\n            result_table["R2"].values + result_table["R2-se"].values,\n            alpha=0.2,\n            color="navy",\n            lw=lw,\n        )\n        plt.legend(loc="best")\n        plt.show()\n\n\n', 'filename': 'C:/Users/00613/Sync-Austausch/_ML/_Orange/Orange-Python-Scripts/Validation_curve.py'}], 'scriptText': '# Plot Validation Curve\n#######################\n# Settings:\nscore = "MSE"                # "R2" or "MSE"; only for regression\nsave_results = False        # True: save Excelfile with results; False: don\'t save\nfile_path = "E:/Downloads/" # file path for save_results\n#                             e.g. "E:/Downloads/" - with slash (!) also in Windows (and trailing slash)\n#####################################################\n# File: Validation_curve.py\n\n"""\n* Widget input: Predictions from Test &amp; Score with Cross validation\n  and optional: Predictions from Test &amp; Score with test on training data\n* Widget output: -\n\n* Computes and shows validation curve for learners out of cross validation by widget Test &amp; Score\n* Uses standard error to indicate variability of the results.\n* Scores are CA (classification accuracy) for classification and R_squared (R2) or MSE for regression.\n* Uses names of learners as x-labels.\n\nUsage: \n* mainly to compare different learner settings for manual hyper parameter optimization.\n* e.g. several learners of the same type, but with different values of a certain hyper parameter\n* Use name in learner widget to indicate different learners\n\n"""\n\nimport numpy as np\nfrom Orange.data import Table\nfrom Orange.evaluation import TestOnTestData\nimport Orange\nfrom sklearn import metrics\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\ndata = in_data.copy()\nn = len(data)\ny_true = data.Y\nn_meta_cols = len(data.domain.metas)\n\nif data.domain.class_var.is_continuous:\n    problem = "reg"\n    n_classes = 0\n    n_learner = n_meta_cols-1\nelse:\n    problem = "class"\n    n_classes = len(data.domain.class_var.values)\n    n_learner = int((n_meta_cols-1)/(n_classes+1))\n\nif in_object == None:\n    show_traindata = False\nelse:\n    show_traindata = True\n    traindata = in_object.copy()\n\nfolds = data.metas[:, n_learner*(n_classes+1)]  # column with folds-indicator\nk = len(np.unique(folds))  # number of folds\n\nca = np.zeros([n_learner,k])\nmse= np.zeros([n_learner,k])\nr2 = np.zeros([n_learner,k])\nr2_total = np.zeros([n_learner])\nca_train = np.zeros([n_learner,k])\nmse_train = np.zeros([n_learner,k])\nr2_train = np.zeros([n_learner,k])\n\n# Split data in folds and calculate metrics\n# -----------------------------------------\nif problem == "class":\n    for fold in range(k):\n        y_test = y_true[folds==fold].astype(int)\n        y_pred_test = data.metas[:,0:n_learner][folds==fold].astype(int)\n        for learner in range(n_learner):  # calculate metrics\n            # Metrics see https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n            # use only CA to avoid problems with multiclass labels\n            ca[learner, fold] = metrics.accuracy_score(y_test, y_pred_test[:, learner])\n\nelif problem == "reg":\n    for fold in range(k):\n        y_test = y_true[folds==fold]\n        y_pred_test = data.metas[:,0:n_learner][folds==fold]\n        for learner in range(n_learner):\n            mse[learner,fold] = metrics.mean_squared_error(y_test, y_pred_test[:, learner])\n            r2[learner,fold] = metrics.r2_score(y_test, y_pred_test[:, learner])\n            r2_total[learner] = metrics.r2_score(y_true, data.metas[:, learner])\n\nif show_traindata: \n    if problem == "class":\n        y_train = traindata.Y.astype(int)\n        y_pred_train = traindata.metas[:,0:n_learner].astype(int)\n        for learner in range(n_learner):\n            ca_train[learner] = metrics.accuracy_score(y_train, y_pred_train[:, learner])\n    elif problem == "reg":\n        y_train = traindata.Y\n        y_pred_train = traindata.metas[:,0:n_learner]\n        for learner in range(n_learner):\n            mse_train[learner] = metrics.mean_squared_error(y_train, y_pred_train[:, learner])\n            r2_train[learner] = metrics.r2_score(y_train, y_pred_train[:, learner])\n\n# Aggregate results (means and standard errors)\n#----------------------------------------------\nresult_table = pd.DataFrame([])\n#result_table["Models"] = [F"{learners[learner]}" for learner,_ in enumerate(learners)]\nresult_table["Models"] = [F"{data.domain.metas[learner]}" for learner in range(n_learner)]\n#print(result_table["Models"])\nif problem == "class":\n    result_table["CA"] = [ca[learner].mean() for learner in range(n_learner)]\n    result_table["CA-se"] = [ca[learner].std(ddof=1)/k for learner in range(n_learner)]\n    if show_traindata: result_table["CA_train"] = [ca_train[learner].mean() for learner in range(n_learner)]\n    \nif problem == "reg":\n    result_table["MSE"] = [mse[learner].mean() for learner in range(n_learner)]\n    result_table["MSE-se"] = [mse[learner].std(ddof=1)/k for learner in range(n_learner)]\n    result_table["R2"] = [r2[learner].mean() for learner in range(n_learner)]\n    result_table["R2"] = r2_total\n    result_table["R2-se"] = [r2[learner].std(ddof=1)/k for learner in range(n_learner)]\n    if show_traindata:\n        result_table["MSE_train"] = [mse_train[learner].mean() for learner in range(n_learner)]\n        result_table["R2_train"] = [r2_train[learner].mean() for learner in range(n_learner)]\n\n# Print to table and save data\n#-----------------------------\nprint("---------------------------")\nprint("Results of Cross validation")\nprint("---------------------------")\nprint("Number of instances: ", len(data))\nprint("Number of folds    : ", k)\nprint("")\nif problem == "class":\n    print(result_table)\nif problem == "reg":\n    print(result_table[["Models", "MSE", "MSE-se", "R2", "R2-se"]])\n    if show_traindata: print(result_table[["Models", "MSE_train", "R2_train"]])\n\nif save_results:\n    file_name_path = file_path + "Results-" + datetime.now().strftime("%Y-%m-%d_%H-%M-%S") + ".xlsx"\n    print("Results in: ",file_name_path)\n    with pd.ExcelWriter(file_name_path) as writer:\n        result_table.to_excel(writer)\n\n# Create Diagram\n#---------------\nfig = plt.figure(figsize=(8,6))\nplt.title("Validation Curve")\nlw = 2\nif problem == "class":\n    plt.ylabel("CA")\n    if show_traindata: plt.plot(\n        result_table["Models"].values, result_table["CA_train"].values, label="Training score", color="darkorange", lw=lw\n    )\n    plt.plot(\n        result_table["Models"].values, result_table["CA"].values, label="Cross-validation score", color="navy"\n    )\n    plt.fill_between(\n        result_table["Models"].values,\n        result_table["CA"].values - result_table["CA-se"].values,\n        result_table["CA"].values + result_table["CA-se"].values,\n        alpha=0.2,\n        color="navy",\n        lw=lw,\n    )\n    plt.legend(loc="best")\n    plt.show()\n    \nif problem == "reg":\n    if score == "MSE":\n        plt.ylabel("MSE")\n        if show_traindata: plt.plot(\n            result_table["Models"].values, result_table["MSE_train"].values, label="Training score", color="darkorange", lw=lw\n        )\n        plt.plot(\n            result_table["Models"].values, result_table["MSE"].values, label="Cross-validation score", color="navy"\n        )\n        plt.fill_between(\n            result_table["Models"].values,\n            result_table["MSE"].values - result_table["MSE-se"].values,\n            result_table["MSE"].values + result_table["MSE-se"].values,\n            alpha=0.2,\n            color="navy",\n            lw=lw,\n        )\n        plt.legend(loc="best")\n        plt.show()\n    else:\n        plt.ylabel("R2")\n        if show_traindata: plt.plot(\n            result_table["Models"].values, result_table["R2_train"].values, label="Training score", color="darkorange", lw=lw\n        )\n        plt.plot(\n            result_table["Models"].values, result_table["R2"].values, label="Cross-validation score", color="navy"\n        )\n        plt.fill_between(\n            result_table["Models"].values,\n            result_table["R2"].values - result_table["R2-se"].values,\n            result_table["R2"].values + result_table["R2-se"].values,\n            alpha=0.2,\n            color="navy",\n            lw=lw,\n        )\n        plt.legend(loc="best")\n        plt.show()\n\n\n', 'splitterState': b'\x00\x00\x00\xff\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x01\x9c\x00\x00\x01#\x01\xff\xff\xff\xff\x01\x00\x00\x00\x02\x00', 'vimModeEnabled': False, '__version__': 2}</properties>
	</node_properties>
	<session_state>
		<window_groups />
	</session_state>
</scheme>
